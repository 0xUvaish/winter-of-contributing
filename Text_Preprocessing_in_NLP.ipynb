{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Preprocessing in NLP",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOi5BvRCKGFr8njJaM6j3IX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepeshaburse/winter-of-contributing/blob/Datascience_With_Python/Text_Preprocessing_in_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuSxHkGRcS15"
      },
      "source": [
        "# Data Science with Python: Text Preprocessing in NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg5y71R36Dev"
      },
      "source": [
        "Natural Language Processing is a subfield of data science concerned with interactions between computers and human (natural) languages. Nowadays, companies have large amounts of data, a lot of which is text data. This text data carries a lot of data that can be analyzed and used for different purposes. The topic we choose, order of words, our tone, and a lot of other factors need to be considered in order to understand the text and intention. This data which is very messy and hard to manipulate is called unstructured data.\n",
        "This unstructured data needs to be preprocessed before it can get used in machine learning models, it is essentially the first step in NLP projects. Some of the preprocessing steps are:\n",
        "- Lowercasing\n",
        "- Remove punctuations\n",
        "- Remove words that contain numbers\n",
        "- Tokenization\n",
        "- Spelling correction\n",
        "- Remove stop words\n",
        "- Lemmatization\n",
        "- Stemming\n",
        "- Removing words with only one letter\n",
        "- Regex\n",
        "- Joining the remaining words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atu4qt5cblbD"
      },
      "source": [
        "Let’s look at each one of these in detail:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4nBOmwB8bqK"
      },
      "source": [
        "1.\tRemoving punctuation:\n",
        "\n",
        "This step is straightforward, it is used to remove any kind of punctuation present in the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYJLJQPt6r9g"
      },
      "source": [
        "2.\tLowercasing of text:\n",
        "\n",
        "This is one of the most common steps of preprocessing and used to achieve a level of uniformity in the data.  In some situations, though, it could result in loss of information, for example when a word is completely in uppercase, it could signify intense emotions. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx_F3AaTbrX9"
      },
      "source": [
        "3.\tRemove words that contain numbers:\n",
        "\n",
        "More often than not, numbers in words are typos. This step is used to remove them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydEGH9HBbwQE"
      },
      "source": [
        "4.\tTokenization:\n",
        "\n",
        "In this step we cut the given text into pieces called tokens. You can also remove punctuation along with tokenizing the text. This is easy for a language like English where every word is separated with a space but that isn’t the same for every language. This also may be a little more complex for biomedical data as it will contain lots of words containing hyphens, parentheses, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK9HrvDCbzG4"
      },
      "source": [
        "5.\tSpelling correction:\n",
        "\n",
        "Although we can use NLTK here too, I prefer using the autocorrect library to spell check in Python.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gsVI3Bkb5yp"
      },
      "source": [
        "6.\tRemove stopwords:\n",
        "\n",
        "Stopwords are words that do not add any value to the text analysis. These are usually very commonly used and help in cleaning the data significantly. \n",
        "The library NLTK already has a list of stopwords for English. Examples of stopwords: I, me, the, a, an, in, are, some, etc. In some cases, though, we can’t use the readymade list available in NLTK because our dataset might need them to analyze it, in this situation, we can make our own list of stopwords and use that. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geZLFMuEb7f4"
      },
      "source": [
        "7.\tLemmatization:\n",
        "\n",
        "Lemmatization is the process of reducing a word to its root word. This helps in the standardization of text. It resolves words to their dictionary form (lemma) for which it requires detailed dictionaries in which the algorithm can look into and link words to their corresponding lemmas. \n",
        "Example: The words ‘running’, ‘ran’, ‘runs’ all reduce to ‘run’.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALVXAmn6b-am"
      },
      "source": [
        "8.\tStemming:\n",
        "\n",
        "This is another standardization step. The words are ‘stemmed’ to its base word in this step too. The disadvantage with stemming is that the word may lose its meaning after stemming. \n",
        "Example: The words ‘programming’, ‘programmer’, ‘program’ will be reduced to the base word ‘program’, ‘crazy’ is stemmed to ‘crazi’ and so on. \n",
        "It is because of this that lemmatization is preferred over stemming.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-nJaOVjcBzM"
      },
      "source": [
        "9.\tRemove letters with only one letter:\n",
        "\n",
        "Words with only one letter will hardly ever add value to the text. In some datasets, we might need these words so this step may be skipped sometimes. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qHzdYV4cHSm"
      },
      "source": [
        "10.\tRegex:\n",
        "\n",
        "Regex, short for regular expression, is used to find data following a certain pattern. For example: if we are required to find all the email addresses in the given data, we will write a regex code that finds all the character sequences that follow the pattern of text@text.text. This can prove to be very useful in cleaning of data if we need to remove a certain kind of data from our dataset. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUlGyKkdcI60"
      },
      "source": [
        "11.\tJoining all the words:\n",
        "\n",
        "This step is used to join all the list of words now. This is the final, clean data that can directly be used in machine learning models and analysis!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7bxc5bbcLlD"
      },
      "source": [
        "Outside of these preprocessing steps, we have many more like rare word removal, frequent word removal, rephrasing text, etc. The steps must be chosen based on what dataset you need to work with and what you want to do with the data. \n",
        "I hope this walked you through the concepts of text preprocessing in NLP. \n"
      ]
    }
  ]
}