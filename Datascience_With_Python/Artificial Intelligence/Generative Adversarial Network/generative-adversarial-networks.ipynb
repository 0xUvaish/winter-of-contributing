{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Generative Adversarial Network (GAN)**","metadata":{}},{"cell_type":"markdown","source":"Generative Adversarial Networks (GAN) are applied to generate similar fake data as real ones . It can be images , speech or text. It has two parts:\n\n1. Generator: This is a neural network which will generate images from a random noise.\n2. Discriminator: This neural network asseses those pictures which will fake or real. ","metadata":{}},{"cell_type":"markdown","source":"**Implementation**\n\nIn this code I'm implementing GAN to a  [Pokeman dataset](https://www.kaggle.com/kvpratama/pokemon-images-dataset). These images along with the fake ones will be fed in batches to the Discriminator.Let's take a look at the steps our GAN will follow-\n\nHere, two neural network will compete with each other.\n\n* Here, we will input a random signal into generator and create images for train the discriminator. Discrimintor will give output probabilities based on provided features/images. \n\n* The discriminator will detect the ground truth ( real or fake ) of generated images and return possiblities a number between 0 and 1 . here 0 represent fake and 1 is real.\n\n* The error will be calculated through the backpropagation and every time weight will be updated.\n\nNow let's start to implement it.\n","metadata":{}},{"cell_type":"markdown","source":"**Importing Libaries**\n> This section imports all the necessary libraries to build the model as well as to plot the necessary results.","metadata":{}},{"cell_type":"code","source":"import os \nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as tt\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import make_grid,save_image\nimport cv2\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-09-23T05:43:18.080709Z","iopub.execute_input":"2021-09-23T05:43:18.081039Z","iopub.status.idle":"2021-09-23T05:43:19.379000Z","shell.execute_reply.started":"2021-09-23T05:43:18.080997Z","shell.execute_reply":"2021-09-23T05:43:19.378255Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"Path = \"../input/pokemon-images-dataset/pokemon_jpg\"\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:19.381806Z","iopub.execute_input":"2021-09-23T05:43:19.382054Z","iopub.status.idle":"2021-09-23T05:43:19.389258Z","shell.execute_reply.started":"2021-09-23T05:43:19.382028Z","shell.execute_reply":"2021-09-23T05:43:19.388408Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"In this part,python library is called for path handling.","metadata":{}},{"cell_type":"code","source":"os.listdir(Path)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:19.391276Z","iopub.execute_input":"2021-09-23T05:43:19.391561Z","iopub.status.idle":"2021-09-23T05:43:19.416112Z","shell.execute_reply.started":"2021-09-23T05:43:19.391529Z","shell.execute_reply":"2021-09-23T05:43:19.415211Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"This block mainly involves the preprocessing as well as augmentation of the dataset. At first, we have resized the dataset's images  to (64,64).After that, cropping has been applied. Since we are working in pytorch, we need to convert this numpy arrays into tensor. \"tt.ToTensor\" achieves this task. And then comes the classic preprocessing task called \"Normalization\". Normalization is performed so that deep learning model can easily do the computation. Finally,horizontal flip is performed over the normalized images. To send the images easily into the network,a dataloader has been created with suitable batch size with shuffling enabled.","metadata":{}},{"cell_type":"code","source":"image_size = 64\nbatch_size = 64\nimage_channels = 3\nstats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n\ntrain_ds1 = ImageFolder(Path,transform=tt.Compose([\n    tt.Resize(image_size),\n    tt.CenterCrop(image_size),\n    tt.ToTensor(),\n    tt.Normalize(*stats),\n    tt.RandomHorizontalFlip(p=0.5)\n]))\n\ntrain_dl = DataLoader(train_ds1, batch_size, shuffle=True, num_workers=3, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:19.417893Z","iopub.execute_input":"2021-09-23T05:43:19.418310Z","iopub.status.idle":"2021-09-23T05:43:19.720031Z","shell.execute_reply.started":"2021-09-23T05:43:19.418273Z","shell.execute_reply":"2021-09-23T05:43:19.719236Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def denorm(img_tensors):\n    return img_tensors * stats[1][0] + stats[0][0]","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:19.725611Z","iopub.execute_input":"2021-09-23T05:43:19.725867Z","iopub.status.idle":"2021-09-23T05:43:19.732268Z","shell.execute_reply.started":"2021-09-23T05:43:19.725841Z","shell.execute_reply":"2021-09-23T05:43:19.731363Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"This part does the task of plotting.","metadata":{}},{"cell_type":"code","source":"def show_images(images, nmax=64):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:19.735219Z","iopub.execute_input":"2021-09-23T05:43:19.735667Z","iopub.status.idle":"2021-09-23T05:43:19.743183Z","shell.execute_reply.started":"2021-09-23T05:43:19.735586Z","shell.execute_reply":"2021-09-23T05:43:19.741991Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def show_batch(dl, nmax=64):\n    for images, _ in dl:\n        show_images(images, nmax)\n        break","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:19.745261Z","iopub.execute_input":"2021-09-23T05:43:19.745684Z","iopub.status.idle":"2021-09-23T05:43:19.752482Z","shell.execute_reply.started":"2021-09-23T05:43:19.745645Z","shell.execute_reply":"2021-09-23T05:43:19.751567Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"show_batch(train_dl)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:19.753807Z","iopub.execute_input":"2021-09-23T05:43:19.754310Z","iopub.status.idle":"2021-09-23T05:43:25.186942Z","shell.execute_reply.started":"2021-09-23T05:43:19.754269Z","shell.execute_reply":"2021-09-23T05:43:25.186032Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"This block checks if the cuda is enabled to take advantage of the gpu. Otherwise it will run the code on cpu. This code creates the dataloader for the device as well.","metadata":{}},{"cell_type":"code","source":"def get_default_device():\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n\ndef to_device(data,device):\n    if isinstance(data,(list,tuple)):\n        return [to_device(x,device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    def __init__(self,dl,device):\n        self.dl = dl\n        self.device = device\n    def __iter__(self):\n        for b in self.dl:\n            yield to_device(b,device)\n    def __len__(self):\n        return len(self.dl)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:25.188204Z","iopub.execute_input":"2021-09-23T05:43:25.188594Z","iopub.status.idle":"2021-09-23T05:43:25.200340Z","shell.execute_reply.started":"2021-09-23T05:43:25.188548Z","shell.execute_reply":"2021-09-23T05:43:25.199427Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"device = get_default_device()\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:25.201843Z","iopub.execute_input":"2021-09-23T05:43:25.202586Z","iopub.status.idle":"2021-09-23T05:43:25.213276Z","shell.execute_reply.started":"2021-09-23T05:43:25.202545Z","shell.execute_reply":"2021-09-23T05:43:25.212427Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"This code sends the train dataloader to the device(cuda/cpu). ","metadata":{}},{"cell_type":"code","source":"train_dl = DeviceDataLoader(train_dl, device)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:25.215360Z","iopub.execute_input":"2021-09-23T05:43:25.215875Z","iopub.status.idle":"2021-09-23T05:43:25.222928Z","shell.execute_reply.started":"2021-09-23T05:43:25.215838Z","shell.execute_reply":"2021-09-23T05:43:25.222107Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"This is the core of our tutorial. This is block where the model has been built.Since GAN has basically two networks--Discriminator and Generator. This block builds the discriminator network.As we can see from the code basically there are repeated use of three layers which constitue a single block.If we observe the block carefully,first there is a convolution layer which extracts low level details from the image. And then batchnormalization is applied.For activation function,leakyrelu is applied.In the last layer,we have flattened the tensors and passed them through the sigmoid activation layer.","metadata":{}},{"cell_type":"code","source":"discriminator = nn.Sequential(\n    nn.Conv2d(3,64,kernel_size=4,stride=2,padding=1,bias=False),\n    nn.BatchNorm2d(64),\n    nn.LeakyReLU(0.2,inplace=True),\n    \n    \n    nn.Conv2d(64,128,kernel_size=4,stride=2,padding=1,bias=False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.2,inplace=True),\n    \n    nn.Conv2d(128,256,kernel_size=4,stride=2,padding=1,bias=False),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(0.2,inplace=True),\n    \n    nn.Conv2d(256,512,kernel_size=4,stride=2,padding=1,bias=False),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(0.2,inplace=True),\n    \n    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n    \n    nn.Flatten(),\n    nn.Sigmoid()\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:25.224284Z","iopub.execute_input":"2021-09-23T05:43:25.224763Z","iopub.status.idle":"2021-09-23T05:43:25.258063Z","shell.execute_reply.started":"2021-09-23T05:43:25.224724Z","shell.execute_reply":"2021-09-23T05:43:25.257290Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"discriminator = to_device(discriminator,device)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:25.259297Z","iopub.execute_input":"2021-09-23T05:43:25.259633Z","iopub.status.idle":"2021-09-23T05:43:25.271827Z","shell.execute_reply.started":"2021-09-23T05:43:25.259596Z","shell.execute_reply":"2021-09-23T05:43:25.271031Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"We have used a latent size of 128. Here is the generator block which consists of transposed convolutional layer. And then batch normalization is used.For activation function,Relu is used which is different from discriminator network. Finally,at the end of the generator block,tanh activation layer is applied.","metadata":{}},{"cell_type":"code","source":"latent_size = 128\ngenerator = nn.Sequential(\n    nn.ConvTranspose2d(latent_size,512,kernel_size=4,stride=1,padding=0,bias = False),\n    nn.BatchNorm2d(512),\n    nn.ReLU(True),\n    \n    nn.ConvTranspose2d(512,256,kernel_size=4,stride=2,padding=1,bias = False),\n    nn.BatchNorm2d(256),\n    nn.ReLU(True),\n    \n    nn.ConvTranspose2d(256,128,kernel_size=4,stride=2,padding=1,bias = False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(True),\n    \n    nn.ConvTranspose2d(128,64,kernel_size=4,stride=2,padding=1,bias = False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(True),\n    \n    nn.ConvTranspose2d(64,3,kernel_size=4,stride=2,padding=1,bias = False),\n    nn.Tanh()\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:25.273074Z","iopub.execute_input":"2021-09-23T05:43:25.273429Z","iopub.status.idle":"2021-09-23T05:43:25.307932Z","shell.execute_reply.started":"2021-09-23T05:43:25.273394Z","shell.execute_reply":"2021-09-23T05:43:25.307297Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"In this block,we have tested the generator block with some random latent tensors.The main goal of this block is to see whether the generator works or not. And the main task of the generator is to produce fake images.","metadata":{}},{"cell_type":"code","source":"xb = torch.randn(batch_size, latent_size, 1, 1) # random latent tensors\nfake_images = generator(xb)\nprint(fake_images.shape)\nshow_images(fake_images)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:25.309349Z","iopub.execute_input":"2021-09-23T05:43:25.309679Z","iopub.status.idle":"2021-09-23T05:43:26.114696Z","shell.execute_reply.started":"2021-09-23T05:43:25.309643Z","shell.execute_reply":"2021-09-23T05:43:26.113859Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"generator = to_device(generator,device)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:26.116112Z","iopub.execute_input":"2021-09-23T05:43:26.116633Z","iopub.status.idle":"2021-09-23T05:43:26.127765Z","shell.execute_reply.started":"2021-09-23T05:43:26.116592Z","shell.execute_reply":"2021-09-23T05:43:26.126913Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"This part is the cream of this tutorial where we wll perform the training of this GAN model.Eventually,we will train the discriminator first.For that,we will first clear the gradient of the discriminator network,which is basically initialize the discriminator network with zero weight. And then real images are passed through this.The main task of the discriminator is to distinguish the real images from the fake ones,generated from the generator network.To be precise, we will try to minimize the loss function of the discriminator net. For the loss function, we have used binary cross-entropy to calculate the loss between real and fake images. Proceeding forward, we observe that fake images are passed through this net to calculate fake loss. In the end,both fake and real loss are summed to calculated the total loss.This loss will be used in the backpropagation phase to update the weight of the discriminator network.And this is done in the last part of this block. Finally this block will output some real scores and fake scores.","metadata":{}},{"cell_type":"code","source":"def train_discriminator(real_images, opt_d):\n    # Clear discriminator gradients\n    opt_d.zero_grad()\n\n    # Pass real images through discriminator\n    real_preds = discriminator(real_images)\n    real_targets = torch.ones(real_images.size(0), 1, device=device)\n    real_loss = F.binary_cross_entropy(real_preds, real_targets)\n    real_score = torch.mean(real_preds).item()\n    \n    # Generate fake images\n    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n    fake_images = generator(latent)\n\n    # Pass fake images through discriminator\n    fake_targets = torch.zeros(fake_images.size(0), 1, device=device)\n    fake_preds = discriminator(fake_images)\n    fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n    fake_score = torch.mean(fake_preds).item()\n\n    # Update discriminator weights\n    loss = real_loss + fake_loss\n    loss.backward()\n    opt_d.step()\n    return loss.item(), real_score, fake_score","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:26.129402Z","iopub.execute_input":"2021-09-23T05:43:26.129891Z","iopub.status.idle":"2021-09-23T05:43:26.141697Z","shell.execute_reply.started":"2021-09-23T05:43:26.129854Z","shell.execute_reply":"2021-09-23T05:43:26.140589Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"This block is pretty similar to the block explained above.Here we are training the generator network.The generator network will basically train through the discriminator network. The goal of the generator network is to produce fake images to fool the discriminator. Its like a competition going between generator and discriminator.Through this game, the loss function of both the network will come into an equilibrium point when the generator will learn to produce images which will look perfectly like real ones.Just like the discriminator network,this net will update its weights through loss function.","metadata":{}},{"cell_type":"code","source":"def train_generator(opt):\n    opt.zero_grad()\n    latent = torch.randn(batch_size,latent_size,1,1,device=device)\n    fake_images = generator(latent)\n    preds = discriminator(fake_images)\n    targets = torch.ones(batch_size,1,device = device)\n    loss = F.binary_cross_entropy(preds,targets)\n    loss.backward()\n    opt.step()\n    return loss.item()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:26.143330Z","iopub.execute_input":"2021-09-23T05:43:26.144017Z","iopub.status.idle":"2021-09-23T05:43:26.153917Z","shell.execute_reply.started":"2021-09-23T05:43:26.143974Z","shell.execute_reply":"2021-09-23T05:43:26.153210Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"sample_dir = 'generated'\nos.makedirs(sample_dir, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:26.155695Z","iopub.execute_input":"2021-09-23T05:43:26.156161Z","iopub.status.idle":"2021-09-23T05:43:26.162793Z","shell.execute_reply.started":"2021-09-23T05:43:26.156103Z","shell.execute_reply":"2021-09-23T05:43:26.162120Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"In this block, we will save the images generated from the generator network.In the end, we will plot them to compare with the real images.","metadata":{}},{"cell_type":"code","source":"def save_samples(index, latent_tensors, show=True):\n    fake_images = generator(latent_tensors)\n    fake_fname = 'generated-images-{0:0=4d}.png'.format(index)\n    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n    print('Saving', fake_fname)\n    if show:\n        fig, ax = plt.subplots(figsize=(8, 8))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:26.164274Z","iopub.execute_input":"2021-09-23T05:43:26.164825Z","iopub.status.idle":"2021-09-23T05:43:26.173476Z","shell.execute_reply.started":"2021-09-23T05:43:26.164788Z","shell.execute_reply":"2021-09-23T05:43:26.172447Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"fixed_latent = torch.randn(64, latent_size, 1, 1, device=device)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:26.175026Z","iopub.execute_input":"2021-09-23T05:43:26.175750Z","iopub.status.idle":"2021-09-23T05:43:26.183539Z","shell.execute_reply.started":"2021-09-23T05:43:26.175680Z","shell.execute_reply":"2021-09-23T05:43:26.182836Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"save_samples(0, fixed_latent)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:26.184976Z","iopub.execute_input":"2021-09-23T05:43:26.185517Z","iopub.status.idle":"2021-09-23T05:43:26.969843Z","shell.execute_reply.started":"2021-09-23T05:43:26.185476Z","shell.execute_reply":"2021-09-23T05:43:26.968820Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"This block defines the setting of the training of the GAN networks.Indeed this is the lock, where will define the hyperparameters of the trainig like the optmizer.In respect of this,we will use Adam optimizer for both of our networks.Besides,here will calculate all the relevant scores of the network. ","metadata":{}},{"cell_type":"code","source":"def fit(epochs, lr, start_idx=1):\n    torch.cuda.empty_cache()\n    \n    # Losses & scores\n    losses_g = []\n    losses_d = []\n    real_scores = []\n    fake_scores = []\n    \n    # Create optimizers\n    opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n    opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n    \n    for epoch in range(epochs):\n        for real_images, _ in tqdm(train_dl):\n            # Train discriminator\n            loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)\n            # Train generator\n            loss_g = train_generator(opt_g)\n            \n        # Record losses & scores\n        losses_g.append(loss_g)\n        losses_d.append(loss_d)\n        real_scores.append(real_score)\n        fake_scores.append(fake_score)\n        \n        # Log losses & scores (last batch)\n        print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n            epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n    \n        # Save generated images\n        save_samples(epoch+start_idx, fixed_latent, show=False)\n    \n    return losses_g, losses_d, real_scores, fake_scores","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:26.971263Z","iopub.execute_input":"2021-09-23T05:43:26.971603Z","iopub.status.idle":"2021-09-23T05:43:26.985805Z","shell.execute_reply.started":"2021-09-23T05:43:26.971561Z","shell.execute_reply":"2021-09-23T05:43:26.984772Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"This block defines two very important hyperparameters--learning rate and epoch.And finally, we will fit the netowrk/train the netowork here.After the execution of this block,we will get our loss scores to see the performance of our designed model.","metadata":{}},{"cell_type":"code","source":"lr = 0.0002\nepochs = 500\nhistory = fit(epochs, lr)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:43:26.987222Z","iopub.execute_input":"2021-09-23T05:43:26.987606Z","iopub.status.idle":"2021-09-23T06:01:16.417575Z","shell.execute_reply.started":"2021-09-23T05:43:26.987554Z","shell.execute_reply":"2021-09-23T06:01:16.416502Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"losses_g, losses_d, real_scores, fake_scores = history","metadata":{"execution":{"iopub.status.busy":"2021-09-23T06:01:16.419478Z","iopub.execute_input":"2021-09-23T06:01:16.420088Z","iopub.status.idle":"2021-09-23T06:01:16.425967Z","shell.execute_reply.started":"2021-09-23T06:01:16.420023Z","shell.execute_reply":"2021-09-23T06:01:16.424684Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image\nImage('./generated/generated-images-0184.png')\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T06:01:16.427722Z","iopub.execute_input":"2021-09-23T06:01:16.428213Z","iopub.status.idle":"2021-09-23T06:01:16.455914Z","shell.execute_reply.started":"2021-09-23T06:01:16.428168Z","shell.execute_reply":"2021-09-23T06:01:16.455074Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"The last two blocks involve the plotting of the \"Epoch-loss\" as well as \"Epoch--score/accuracy\" curve. ","metadata":{}},{"cell_type":"code","source":"plt.plot(losses_d, '-')\nplt.plot(losses_g, '-')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['Discriminator', 'Generator'])\nplt.title('Losses');","metadata":{"execution":{"iopub.status.busy":"2021-09-23T06:01:16.457361Z","iopub.execute_input":"2021-09-23T06:01:16.457929Z","iopub.status.idle":"2021-09-23T06:01:16.667852Z","shell.execute_reply.started":"2021-09-23T06:01:16.457890Z","shell.execute_reply":"2021-09-23T06:01:16.666919Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"plt.plot(real_scores, '-')\nplt.plot(fake_scores, '-')\nplt.xlabel('epoch')\nplt.ylabel('score')\nplt.legend(['Real', 'Fake'])\nplt.title('Scores');","metadata":{"execution":{"iopub.status.busy":"2021-09-23T06:01:16.669336Z","iopub.execute_input":"2021-09-23T06:01:16.669757Z","iopub.status.idle":"2021-09-23T06:01:16.841491Z","shell.execute_reply.started":"2021-09-23T06:01:16.669712Z","shell.execute_reply":"2021-09-23T06:01:16.840509Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"So finally we have found the plot fake vs real images ! \n\nThank you.\n[References](https://www.kaggle.com/ibtesama/generative-adversarial-networks-demystified)","metadata":{}}]}