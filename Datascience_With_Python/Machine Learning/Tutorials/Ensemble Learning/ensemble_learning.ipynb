{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ensemble_learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1RwkKnJcf8A"
      },
      "source": [
        "# **ENSEMBLE LEARNING**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-0oY-ThnHrw"
      },
      "source": [
        "Suppose you give an expert a complex problem to solve and he even solves it and gets close to the right answer. And then you give the same problem to hundred people and aggregate their reults. In many cases, you will find that the aggregate of the hundred people's results was better than the result of the expert. This is known as **Wisdom Of Crowd** ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW8WaiVIdTMF"
      },
      "source": [
        "Similarly in Machine learning, if we aggregate the predictions of several predictors then we can get a robust model with better predictions than we can get using a single best predictor. This is known as **Ensemble Learning**. \n",
        "\n",
        "* Ensemble learning is the art of combining diverse set of models together to improvise on the stability and predictive power of the final model.\n",
        "\n",
        "* The group of the predictors is called *Ensemble* and the ensemble learning algorithm is called *Ensemble method*.\n",
        "\n",
        "* Ensemble methods work best when all predictors are independent of one another. Ensemble's accuracy can be enhanced in following ways :\n",
        "  * One way to get diverse predictors is to train them using different algorithms. This way they will produce different types of errors and hence improving ensemble's accuracy.\n",
        "  * Another way of improving the ensemble's accuracy is by using the same training algorithm for every predictor and training them on different subsets of training set.\n",
        "\n",
        "  ![Image.png](https://imgr.search.brave.com/zSW2a2OZfEqN7UOijP_eZRjv5Vx48Jv7ZCFLZ-68uvE/fit/1000/594/ce/1/aHR0cHM6Ly9jZG4t/aW1hZ2VzLTEubWVk/aXVtLmNvbS9tYXgv/MTAwMC8wKmMwRWc2/LVVBcmtzbGd2aXcu/cG5n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjJcWl521rXb"
      },
      "source": [
        "## **Voting Classifier**\n",
        "* **Hard Voting Classifier** : Suppose we have trained a Logistic Regression classifier, an SVM classfier, a Random Forest classifier, a K-Nearest Neighbors. One way to create a better classifier out of these is to aggregate the predictions of the above trained classifiers and predict the class that gets the most votes. This is known as **Hard Voting Classifier**.\n",
        "\n",
        "* **Soft Voting Classifier** : If all the classifiers are able to estimate the probabilities (i.e. they all have *predict_proba()* method) then we can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is known as **Soft Voting Classifier**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HaGti234aUq"
      },
      "source": [
        "#The following code creates a voting classifier of 3 different classifiers\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "svm_clf = SVC(gamma=\"scale\", random_state=42)\n",
        "X,y = make_moons(n_samples=100, noise=0.15)\n",
        "\n",
        "X_train = pd.DataFrame(X, columns = [\" Feature 1\", \" Feature 2\"])\n",
        "y_train = pd.DataFrame(y, columns = [\"Label\"])\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators = [ ('lr',log_clf), ('rf',rnd_clf), ('svc',svm_clf) ],\n",
        "    voting = 'hard')          # voting = 'soft', for Soft Voting Classifier\n",
        "voting_clf.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwCeuR-A_amY"
      },
      "source": [
        "**Bootstrapping** : In statistics, bootstrapping refers to a resample method that consists of repeatedly drawn, with replacement, samples from data to form other smaller datasets, called bootstrapping samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VUUbtwDBDcK"
      },
      "source": [
        "## **Standard Ensemble Methods** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewUZq8Ec-Hyj"
      },
      "source": [
        "* **Bagging** : Bagging means bootstrap+aggregating and it is a ensemble method in which we first bootstrap our data with replacement and for each bootstrap sample we train one model. After that, we aggregate them with equal weights. It is also known as *Bootstrap aggregating.*\n",
        "\n",
        "![image.png](https://machinelearningmastery.com/wp-content/uploads/2020/11/Bagging-Ensemble.png)\n",
        "* **Pasting** : Pasting is same as that bagging. But here the sampling is done without replacement. \n",
        "\n",
        "**Note** : Only Bagging allows training instances to be sampled several times for the same predictor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xt_uyScBFIb"
      },
      "source": [
        "#The following code trains an ensemble of 100 Decision Tree Classifiers and each is trained on 100 training samples with replacement, hence Bagging.\n",
        "#For Pasting:(bootstrap=False) \n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators=500,\n",
        "    max_samples=100, bootstrap=True, random_state=42)\n",
        "bag_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHFimDItjoVN"
      },
      "source": [
        "* **Stacking** : Stacking is a general procedure where a predictor is trained to combine the individual predictors. Here, the individual predictors are called the first-level predictors, while the combiner is called the second-level predictor, or meta-predictor. ![image.png](https://imgr.search.brave.com/czA8_GBFUi5t8elRPeJQa-H-znnVrmwFhbRIu5s-VgQ/fit/946/400/ce/1/aHR0cHM6Ly8xLmJw/LmJsb2dzcG90LmNv/bS8tUzhzcy16VmZw/Uk0vVjFxS2N4ZkN2/TkkvQUFBQUFBQUFE/MEkvOFVVRnlyRTRN/cVFZWXVXU3hyT092/WDN6UmZ3OTNuQ0x3/Q0xjQi9zMTYwMC9T/dGFja2luZy5wbmc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDme8a-Dk8mr"
      },
      "source": [
        "**Boosting** : In Boosting the models are fit and added to the ensemble sequentially such that the second model attempts to correct the predictions of the first model, the third corrects the second model, and so on. The learning algorithm is modified to pay more or less attention to specific examples (rows of data) based on whether they have been predicted correctly or incorrectly by previously added ensemble members. \n",
        "\n",
        "\n",
        "![Image.png](https://imgr.search.brave.com/YoCQ6o7m6cX0iKefwvBjjn04X9VyL1mq-gMJkMvIMZc/fit/1200/1200/ce/1/aHR0cHM6Ly93d3cu/ZWR1cmVrYS5jby9i/bG9nL3dwLWNvbnRl/bnQvdXBsb2Fkcy8y/MDE5LzA2L1doYXQt/SXMtQm9vc3Rpbmct/Qm9vc3RpbmctTWFj/aGluZS1MZWFybmlu/Zy1FZHVyZWthLW1p/bi5wbmc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jelVfq-jKhKK"
      },
      "source": [
        "# **Random Forest** \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3vzh2hoKhTV"
      },
      "source": [
        "* A random forest is an ensemble of decision trees generally trained via the bagging method that can be used for classification or regression. In most cases, it is used bagging. Each tree in the forest outputs a prediction and the most voted becomes the output of the model. This is helpful to make the model with more accuracy and stable, preventing overfitting.\n",
        "\n",
        "* The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. The algorithm resuls in greater tree diversity, which trades a higher bias for a lower variance, generally yielding an overall better model\n",
        "\n",
        "![Image.png](https://imgr.search.brave.com/dN4ZEdBFdELS0WAE61gKg9ZzKGXXl23o5NWTIppprzo/fit/741/502/ce/1/aHR0cHM6Ly9taXJv/Lm1lZGl1bS5jb20v/bWF4LzE0ODIvMCpT/cmc3aHRqNFRPTVA1/bGRYLnBuZw)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zgFbJRgOv9W"
      },
      "source": [
        "# The following code is an implementation of Random Forest Classifier\n",
        "# This Random Classifier uses 500 decision trees with maximum 16 nodes and fitting it to the moon training set that we created in above code blocks\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
        "rnd_clf.fit(X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFnX1CYaPi2D"
      },
      "source": [
        "# **Conclusion**\n",
        "\n",
        "* The goal of any machine learning problem is to find a single model that will best fit to our training dataset and make best future predictions. Rather than making one model and hoping this model is the best/most accurate predictor we can make, ensemble methods take a myriad of models into account, and average those models to produce one final model. \n",
        "\n",
        "* Above, we discussed in detail about Ensemble Learning and also discussed various ensemble strategies and used *Moon Dataset* as our training dataset for our models.\n",
        "\n",
        "* While ensemble learning is a very powerful tool, it also has some tradeoffs.\n",
        "Using ensemble means you must spend more time and resources on training your machine learning models. For instance, a random forest with 500 trees provides much better results than a single decision tree, but it also takes much more time to train. Running ensemble models can also become problematic if the algorithms you use require a lot of memory."
      ]
    }
  ]
}