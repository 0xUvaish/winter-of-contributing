# **Regularization in Regression**

####  **What is Regularization ?**


In mathematics, statistics, finance, computer science, particularly in machine learning and inverse problems, regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting.

#### **Why do we need Regularization ?**

Regularization is used in machine learning models to cope with the problem of overfitting i.e. when the difference between training error and the test error is too high.

#### **Regularization in Regression**
There are mainly two types of regularization techniques:


- Lasso Regression
- Ridge Regression

#### **Lasso Regression**
A regression model which uses L1 Regularization technique is called LASSO(Least Absolute Shrinkage and Selection Operator) regression.
#### **Ridge Regression**
A regression model that uses L2 regularization technique is called Ridge regression. 
Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function(L)


- For the Dataset being used [Click here](https://www.kaggle.com/quantbruce/real-estate-price-prediction).

#### **Libraries Used**

- pandas
- sklearn

#### **Screenshots**

1[]()


