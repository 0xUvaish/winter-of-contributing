<b>Topic: Apriori Algorithm for data mining.</b>

<b>Content</b>
1. What are itemsets and frequent itemsets
2. What does apriori algorithm do?
3. Important terms
4. Steps
5. Example
6. Limitations

<b>What are itemsets and frequent itemsets?</b><br>
When different kinds of items together form a set, it is called an itemset. If it contains k different types of items, then it is precisely called 'k-itemset'.When in a certain amount of transactions, a particular itemset is occuring frequently, it is called a frequent itemset.<br>
Now the question arises, how can we know if an itemset is occuring frequently? This is measured with the help of a quantity termed as "Minimum support threshold".
If number of occurance of an itemset is greater than or equal to this specified threshold, it is said to be occuring frequently.<br><br>

<b>What does Apriori algorithm do?</b><br>
Apriori algorithm discovers relation between different items in a dataset.It checks on which items together are forming strong association rules and hence can be called frequent itemsets. This result is useful for many real world problems including market basket analysis,health pattern identification, recommendation systems etc.<br><br>

<b>Important terms</b><br>
There are a few terms that we have to be familiar with before proceeding to understand the algorithm.They are listed here:<br>
<b>1.Support</b> - Suppose the support percentage of item x and y is given as 2%, it means that in 2% of the total transactions x and y were bought together. <br>
<b>2.Confidence</b> - Suppose the confidence percentage of items x and y is given to be 60%, then it means that about 60% customers who bought x as well as y.<br>
Both these quantities can be calculated as follows:<br>
![image1](https://www.softwaretestinghelp.com/wp-content/qa/uploads/2019/09/Support-and-Confidence-for-Itemset-A-and-B.png)<br>
<b>3.Join</b> - The join step find all the occurances of a k-itemset in every iteration<br>
<b>4.Prune</b> - This step scans the count of each item in the database. If the candidate item does not meet minimum support, then it is regarded as infrequent and thus it is removed.<br>

<b>Steps</b><br>
<b>#1</b> In the first iteration of the algorithm, each item is taken as a 1-itemsets candidate. The algorithm will count the occurrences of each item.

<b>#2</b> Let there be some specified minimum support, say 2. The set of 1 – itemsets whose occurrence is satisfying the min sup are determined. Only those candidates which count more than or equal to 2, are taken ahead for the next iteration and the others are pruned.

<b>#3</b> Next, 2-itemset frequent items with min_sup are discovered. For this in the join step, the 2-itemset is generated by forming a group of 2 by combining items with itself.

<b>#4</b> The 2-itemset candidates are pruned using min-sup threshold value. Now the table will have 2 –itemsets with min-sup only.

<b>#5</b> This process goes on with increasing number of k in k-itmesets until the most frequent itemset is achieved.<br><br>
<b>EXAMPLE</b><br>
Let us understand this by the following example.<br>
Let their be a set of 6 transactions of items l1,l2,l3,l4,l5. We have specified min_sup=3 and minimun confidence level to be 60%.<br>
![image2](https://github.com/rachita11/winter-of-contributing/blob/Datascience_With_Python/Datascience_With_Python/Machine%20Learning/Tutorials/Apriori%20Algorithm/images/table%201.png)
<br>
1. First we find the number of occurances of each item in these transactions and table it down.
![image3](https://github.com/rachita11/winter-of-contributing/blob/Datascience_With_Python/Datascience_With_Python/Machine%20Learning/Tutorials/Apriori%20Algorithm/images/table%202.png)<br>
2. We can see that some of the list elements have a count less than min_sup of value 3. Hence, we will prune those items and make a new table.
 ![image4](https://github.com/rachita11/winter-of-contributing/blob/Datascience_With_Python/Datascience_With_Python/Machine%20Learning/Tutorials/Apriori%20Algorithm/images/table%203.png)<br>
3. Now using these many items, we will join 2-itemsets and find out their counts.
![image5](https://github.com/rachita11/winter-of-contributing/blob/Datascience_With_Python/Datascience_With_Python/Machine%20Learning/Tutorials/Apriori%20Algorithm/images/table%204.png)<br>
4. Again in the next step, the rows having less than required count will get pruned, giving us the table below.
![image6](https://github.com/rachita11/winter-of-contributing/blob/Datascience_With_Python/Datascience_With_Python/Machine%20Learning/Tutorials/Apriori%20Algorithm/images/table%205.png)<br>
5. The join process will take place, this time with 3-itemsets and we calculate their count and get the table shown below.
![image7](https://github.com/rachita11/winter-of-contributing/blob/Datascience_With_Python/Datascience_With_Python/Machine%20Learning/Tutorials/Apriori%20Algorithm/images/table%206.jpg)<br>
6. We observe that only one itemset <b>{l1 l2 l3}</b> pass the criteria of having count 3 and hence hece only that itemset can be called a frequent itemset.<br>

We now apply the association rules to find if they surpass the minimum confidence value or not.
![image8](https://github.com/rachita11/winter-of-contributing/blob/Datascience_With_Python/Datascience_With_Python/Machine%20Learning/Tutorials/Apriori%20Algorithm/images/rules.png)
This shows that all the above association rules are strong if minimum confidence threshold is 60%.

<b>Limitations of this algorithm</b><br>
1.It requires high computation if the itemsets are very large and the minimum support is kept very low.<br>
2.The entire database needs to be scanned.<br>





