{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_7_3_LGBM_ (D).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_QG9qxZtrKE"
      },
      "source": [
        "# **LGBM [ Light Gradient Boosted Machine ]**\n",
        "<BR>\n",
        "\n",
        "<img src=\"https://repository-images.githubusercontent.com/64991887/dc855780-e34b-11ea-9ab8-e08ca33288b0\">\n",
        "\n",
        "<br>\n",
        "\n",
        "## **What is LGBM ?**\n",
        "\n",
        "* Gradient boosting refers to a class of ensemble machine learning algorithms that can be used for classification or regression predictive modeling problems.\n",
        "* LightGBM is a gradient boosting framework based on decision trees to increases the efficiency of the model and reduces memory usage.\n",
        "* LightGBM is called “Light” because of its computation power and giving results faster. It takes less memory to run and is able to deal with large amounts of data. \n",
        "* It uses two techniques: **Gradient-based One Side Sampling** and **Exclusive Feature Bundling (EFB)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTjnmFXC2njW"
      },
      "source": [
        "##**How it differs from other tree based algorithm?**\n",
        "\n",
        "* Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise.\n",
        "* It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm.\n",
        "* Below diagrams explain the implementation of LightGBM \n",
        "\n",
        "<img src=\"https://datascience.eu/wp-content/uploads/2019/12/Screenshot-2020-10-21-at-18.12.57.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KVgqefpxrX5"
      },
      "source": [
        "##**When is LGBM used ?**\n",
        "LightGBM is not for a small volume of datasets. It can easily overfit small data due to its sensitivity. It can be used for data having more than 10,000+ rows. There is no fixed threshold that helps in deciding the usage of LightGBM. It can be used for large volumes of data especially when one needs to achieve a high accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV3uzSNs3a7y"
      },
      "source": [
        "## **How is it Implemented ?**\n",
        "\n",
        "* Implementation of Light GBM is easy, the only complicated thing is parameter tuning.\n",
        "* It is very important for an implementer to know atleast some basic parameters of Light GBM.\n",
        "* Hence below are the impotant parameters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZmFHOnp3_YB"
      },
      "source": [
        "##**Parameters :**\n",
        "\n",
        "* LightGBM has more than 100 parameters that are given in the documentation of LightGBM.\n",
        "* Few important parameters and their usage is listed below :\n",
        "\n",
        "--> **max_depth** : It sets a limit on the depth of tree. The default value is 20. It is effective in controlling over fitting.\n",
        "\n",
        "--> **categorical_feature** : It specifies the categorical feature used for training model.\n",
        "\n",
        "--> **bagging_fraction** : It specifies the fraction of data to be considered for each iteration.\n",
        "\n",
        "--> **early_stopping_round**: This parameter can help you speed up your analysis. Model will stop training if one metric of one validation data doesn’t improve in last early_stopping_round rounds. This will reduce excessive iterations.\n",
        "\n",
        "--> **lambda**: lambda specifies regularization. Typical value ranges from 0 to 1.\n",
        "\n",
        "--> **num_iterations** : It specifies the number of iterations to be performed. The default value is 100.\n",
        "\n",
        "--> **num_leaves** : It specifies the number of leaves in a tree. It should be smaller than the square of max_depth.\n",
        "\n",
        "--> **max_bin** : It specifies the maximum number of bins to bucket the feature values.\n",
        "\n",
        "--> **min_data_in_bin** : It specifies minimum amount of data in one bin.\n",
        "\n",
        "--> **task** : It specifies the task we wish to perform which is either train or prediction. The default entry is train. Another possible value for this parameter is prediction.\n",
        "\n",
        "--> **feature_fraction** : It specifies the fraction of features to be considered in each iteration. The default value is one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECDUCiJO6KDO"
      },
      "source": [
        "##**Practical Implementation**\n",
        " \n",
        "For quick implementation of the algorithm scikit-lean’s wrapper is used for the classifier.\n",
        "\n",
        "As always, it starts by importing the model:\n",
        "\n",
        "    from lightgbm import LGBMClassifier\n",
        "\n",
        "\n",
        "The next step is to create an instance of the model while setting the objective. The options for the objective are regression for LGBMRegressor, binary or multi-class for LGBMClassifier, and LambdaRank for LGBMRanker.\n",
        "\n",
        "    model = LGBMClassifier(objective=’multiclass’)\n",
        "\n",
        "\n",
        "When fitting the model, categorical features can be set as follow:\n",
        "\n",
        "    model.fit(X_train,y_train,categorical_feature=[0,3])\n",
        "\n",
        "\n",
        "Once predictions are run on the model, one canobtain the important features:\n",
        "\n",
        "    predictions = model.predict(X_test)importances = model.feature_importances_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynsGnWxEAwgZ"
      },
      "source": [
        "Let us take an example describing implementation of LGBM The following kernel shows how to run a regression using LightGBM. It also runs some feature engineering to improve the score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dPH1QSeCl_i"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "print('Loading data...')\n",
        "# load or create your dataset\n",
        "regression_example_dir = Path(__file__).absolute().parents[1] / 'regression'\n",
        "df_train = pd.read_csv(str(regression_example_dir / 'regression.train'), header=None, sep='\\t')\n",
        "df_test = pd.read_csv(str(regression_example_dir / 'regression.test'), header=None, sep='\\t')\n",
        "\n",
        "y_train = df_train[0]\n",
        "y_test = df_test[0]\n",
        "X_train = df_train.drop(0, axis=1)\n",
        "X_test = df_test.drop(0, axis=1)\n",
        "\n",
        "# create dataset for lightgbm\n",
        "lgb_train = lgb.Dataset(X_train, y_train)\n",
        "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
        "\n",
        "# specify your configurations as a dict\n",
        "#specifying parameters\n",
        "params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'regression',\n",
        "    'metric': {'l2', 'l1'},\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "print('Starting training...')\n",
        "# training model\n",
        "gbm = lgb.train(params,\n",
        "                lgb_train,\n",
        "                num_boost_round=20,\n",
        "                valid_sets=lgb_eval,\n",
        "                early_stopping_rounds=5)\n",
        "\n",
        "print('Saving model...')\n",
        "# save model to file\n",
        "gbm.save_model('model.txt')\n",
        "\n",
        "print('Starting predicting...')\n",
        "# predict\n",
        "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
        "# eval\n",
        "rmse_test = mean_squared_error(y_test, y_pred) ** 0.5\n",
        "print(f'The RMSE of prediction is: {rmse_test}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPJXzCSMDEze"
      },
      "source": [
        "Hence in the above mentioned way , LGBM algo is implemented !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFHY1aOQ5t85"
      },
      "source": [
        "##**LightGBM Advantages**\n",
        " \n",
        "According to the official docs, here are the advantages of the LightGBM framework:\n",
        "\n",
        "* Faster training speed and higher efficiency\n",
        "* Lower memory usage\n",
        "* Better accuracy\n",
        "* Support of parallel and GPU learning\n",
        "* Capable of handling large-scale data\n",
        "\n",
        "<br>\n",
        "\n",
        "##**LightGBM Applications**\n",
        " \n",
        "LightGBM can be best applied to the following problems:\n",
        "\n",
        "* Binary classification using the logloss objective function\n",
        "* Regression using the L2 loss\n",
        "* Multi-classification\n",
        "* Cross-entropy using the logloss objective function\n",
        "* LambdaRank using lambdarank with NDCG as the objective function\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICSrEUF87NcJ"
      },
      "source": [
        "##**Conclusion**\n",
        " \n",
        "\n",
        "* Hence LightGBM is considered to be a really fast algorithm and the most used algorithm in machine learning when it comes to getting fast and high accuracy results.\n",
        " \n",
        "\n",
        "* In this Documentation, I have tried to give you the basic idea about the algorithm, different parameters that are used in the LightGBM algorithm.Hope it gives you brief understanding about the same !\n",
        "\n",
        "####**References**\n",
        "\n",
        "* https://lightgbm.readthedocs.io/en/latest/index.html\n",
        "* https://sites.google.com/view/lauraepp/parameters\n",
        "* https://neptune.ai/blog/lightgbm-parameters-guide"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udfDYqMz70aF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}