{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_6_1_Singular Value Decomposition_(D).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Nox00LMbD-g"
      },
      "source": [
        "# **Singular Value Decomposition**\n",
        "\n",
        "Singlular Value Decomposition (SVD) is one of the concepts od linear algebra and machine learning. SVD allows us to extract and untangle information. SVD gives you the whole nine-yard of diagonalizing a matrix into special matrices that are easy to manipulate and to analyze. \n",
        "\n",
        "<br>\n",
        "\n",
        "## **Singular vectors and Singular values**\n",
        "\n",
        "The matrix AAᵀ and AᵀA are very special in linear algebra. Consider any m × n matrix A, we can multiply it with Aᵀ to form AAᵀ and AᵀA separately. These matrices are:  \n",
        "\n",
        "* symmetrical,\n",
        "\n",
        "* square,\n",
        "\n",
        "* at least positive semidefinite (eigenvalues are zero or positive),\n",
        "\n",
        "* both matrices have the same positive eigenvalues,\n",
        "\n",
        "* both have the same rank r as A.\n",
        "\n",
        "In addition, the covariance matrices that we often use in ML are in this form. Since they are symmetric, we can choose its eigenvectors to be orthonormal (perpendicular to each other with unit length) — this is a fundamental property for symmetric matrices.\n",
        "\n",
        "![vector-imgae](https://miro.medium.com/max/1050/1*fDMLG40hhRi4gkQBiPPk5w.jpeg)\n",
        "\n",
        "Let’s introduce some terms that frequently used in SVD. We name the eigenvectors for AAᵀ as uᵢ and AᵀA as vᵢ here and call these sets of eigenvectors u and v the singular vectors of A. Both matrices have the same positive eigenvalues. The square roots of these eigenvalues are called singular values.  \n",
        "Not too many explanations so far but let’s put everything together first and the explanations will come next. We concatenate vectors uᵢ into U and vᵢ into V to form orthogonal matrices.  \n",
        "![g](https://miro.medium.com/max/1050/1*WNk8KMCbWeEg8rvNBM2gpg.gif)\n",
        "\n",
        "Since these vectors are orthonormal, it is easy to prove that U and V obey\n",
        "![r](https://miro.medium.com/max/1050/1*OoMBe1LoSziciLoWpzGAdQ.jpeg)\n",
        "\n",
        "## **SVD**\n",
        "\n",
        "Let’s start with the hard part first. SVD states that any matrix A can be factorized as:\n",
        "![e](https://miro.medium.com/max/1050/1*WIAHFWAg03FFvNJdun9W0w.jpeg)\n",
        "\n",
        "where U and V are orthogonal matrices with orthonormal eigenvectors chosen from AAᵀ and AᵀA respectively. S is a diagonal matrix with r elements equal to the root of the positive eigenvalues of AAᵀ or Aᵀ A (both matrics have the same positive eigenvalues anyway). The diagonal elements are composed of singular values.\n",
        "\n",
        "![a](https://miro.medium.com/max/1050/1*MJNokn4E9dBLrncETGIhHg.jpeg)\n",
        "\n",
        "i.e. an m× n matrix can be factorized as:\n",
        "![f](https://miro.medium.com/max/1050/1*tmVzY_1k9_JpxyKDkXwAQA.jpeg)\n",
        "\n",
        "![z](https://miro.medium.com/max/1050/1*n5pHiLBbkM2kza1OFUsLNg.jpeg)\n",
        "\n",
        "We can arrange eigenvectors in different orders to produce U and V. To standardize the solution, we order the eigenvectors such that vectors with higher eigenvalues come before those with smaller values.\n",
        "\n",
        "\n",
        "## **Example**\n",
        "\n",
        "Before moving forward, lets demonstrate what we have have learned so far with a simple example.\n",
        "\n",
        "![a](https://miro.medium.com/max/1050/1*4DncmDEnF9SIYTTrDR0Adw.png)\n",
        "\n",
        "These matrices are at least positive semidefinite (all eigenvalues are positive or zero). As shown, they share the same positive eigenvalues (25 and 9). The figure below also shows their corresponding eigenvectors.\n",
        "\n",
        "![b](https://miro.medium.com/max/1050/1*RG2fiVNyPDr77eS4qsAjLg.jpeg)\n",
        "\n",
        "The singular values are the square root of positive eigenvalues, i.e. 5 and 3. Therefore, the SVD composition is\n",
        "\n",
        "![c](https://miro.medium.com/max/1050/1*xnlAa8E-c63HnMcTRcb5HA.jpeg)\n",
        "\n",
        "\n",
        "## **Proof of SVD**\n",
        "\n",
        "To proof SVD, we want to solve U, S, and V with:\n",
        "\n",
        "![d](https://miro.medium.com/max/1050/1*ZrydTbVPycTfNGULVTPSeg.jpeg)\n",
        "\n",
        "We have 3 unknowns. Hopefully, we can solve them with the 3 equations above. The transpose of A is\n",
        "\n",
        "![e](https://miro.medium.com/max/1050/1*CB2zCTb7mZwSiy7Q49mcUA.jpeg)\n",
        "\n",
        "Knowing\n",
        "\n",
        "![f](https://miro.medium.com/max/1050/1*otGyHsvtFLtjW42iKKaTkQ.jpeg)\n",
        "\n",
        "We compute AᵀA,\n",
        "\n",
        "![g](https://miro.medium.com/max/1050/1*qCSYDYEB1CEVSXmZfQ4BkA.jpeg)\n",
        "\n",
        "The last equation is equilvant to the eigenvector definition for the matrix (AᵀA). We just put all eigenvectors in a matrix.\n",
        "\n",
        "![h](https://miro.medium.com/max/1050/1*tTHpTWQtjb6uxqremadCZQ.jpeg)\n",
        "\n",
        "with VS² equals\n",
        "\n",
        "![i](https://miro.medium.com/max/1050/1*Yq7bXDClX1Qwe42M-t4djw.jpeg)\n",
        "\n",
        "V hold all the eigenvectors vᵢ of AᵀA and S hold the square roots of all eigenvalues of AᵀA. We can repeat the same process for AAᵀ and come back with a similar equation.\n",
        "\n",
        "![j](https://miro.medium.com/max/1050/1*vpWeR_dAqd4gnI9IxnJMOQ.gif)\n",
        "\n",
        "Hence, Proved.\n",
        "\n",
        "## **Reformulate SVD**\n",
        "\n",
        "Since matrix V is orthogonal, VᵀV equals I. We can rewrite the SVD equation as:\n",
        "\n",
        "![k](https://miro.medium.com/max/1050/1*0LEG-KOZYYYsaXnQxMCkXA.gif)\n",
        "\n",
        "This equation establishes an important relationship between uᵢ and vᵢ.  \n",
        "<br>\n",
        "\n",
        "Recall\n",
        "\n",
        "![l](https://miro.medium.com/max/1050/1*3xIyvsDwrkGa_YL87eBtBg.gif)\n",
        "\n",
        "Apply AV = US,\n",
        "\n",
        "![m](https://miro.medium.com/max/1050/1*KGcqnL20ihPN4RDyLhQzXA.jpeg)\n",
        "\n",
        "This can be generalized as,\n",
        "\n",
        "![n](https://miro.medium.com/max/1050/1*77EJGbPLWXtUtYx6-a4Gdg.gif)\n",
        "\n",
        "Recall,\n",
        "\n",
        "![o](https://miro.medium.com/max/1050/1*fWBhde6AExVN42MgkJIG9A.jpeg)\n",
        "\n",
        "and\n",
        "\n",
        "![p](https://miro.medium.com/max/1050/1*CTcIbBs6SVx_Zv_RhnL0xQ.gif)\n",
        "\n",
        "The SVD decomposition can be recognized as a series of outer products of uᵢ and vᵢ.\n",
        "\n",
        "![q](https://miro.medium.com/max/1050/1*0n2-o06c_j42d0MJo7igYQ.gif)\n",
        "\n",
        "This formularization of SVD is the key to understand the components of A. It provides an important way to break down an m × n array of entangled data into r components. Since uᵢ and vᵢ are unit vectors, we can even ignore terms (σᵢuᵢvᵢᵀ) with very small singular value σᵢ.  \n",
        "<br>\n",
        "\n",
        "Let’s first reuse the example before and show how it works.\n",
        "\n",
        "![r](https://miro.medium.com/max/1050/1*4DncmDEnF9SIYTTrDR0Adw.png)\n",
        "\n",
        "The matrix A above can be decomposed as\n",
        "\n",
        "![s](https://miro.medium.com/max/1050/1*5Kzrmdw7c7X7hFuvRXTMOg.gif)\n",
        "\n",
        "## **Moore-Penrose Pseudoinverse**\n",
        "\n",
        "For a linear equation system, we can compute the inverse of a square matrix A to solve x.\n",
        "\n",
        "![t](https://miro.medium.com/max/1050/1*a1inq-_XL9WHTCsxzHpamQ.jpeg)\n",
        "\n",
        "But not all matrices are invertible. Also, in ML, it will be unlikely to find an exact solution with the presence of noise in data. Our objective is to find the model that best fit the data. To find the best-fit solution, we compute a pseudoinverse\n",
        "\n",
        "![u](https://miro.medium.com/max/1050/1*PbhfkyF7h7_e6bbWp6uPIw.jpeg)\n",
        "\n",
        "which minimizes the least square error below.\n",
        "\n",
        "![v](https://miro.medium.com/max/1050/1*aQ2MqySUZnIbCIrxfH1G8Q.png)\n",
        "\n",
        "And the solution for x can be estimated as,\n",
        "\n",
        "![w](https://miro.medium.com/max/1050/1*lF1z-LodZHA3834kseswYw.jpeg)\n",
        "\n",
        "In a linear regression problem, x is our linear model, A contains the training data and b contains the corresponding labels. We can solve x by\n",
        "\n",
        "![x](https://miro.medium.com/max/1050/1*ClzObIIjZyQDb9svX4FjjQ.jpeg)\n",
        "\n",
        "![y](https://miro.medium.com/max/1050/1*mu9Z_NVYd_3CXwhbERVB8Q.jpeg)\n",
        "\n",
        "Here is an example,\n",
        "![z](https://miro.medium.com/max/1050/1*xxatolWVNPjMCUEEWLfvyg.jpeg)\n",
        "\n",
        "## **Variance & covariance**\n",
        "\n",
        "In ML, we identify patterns and relationship. How do we identify the correlation of properties in data? Let’s start the discussion with an example. We sample the height and weight of 12 people and compute their means. We zero-center the original values by subtracting them with its mean. For example, Matrix A below holds the adjusted zero-centered height and weight.\n",
        "\n",
        "![aa](https://miro.medium.com/max/1050/1*QPWbq3WpMHSHluFLK2yXyg.gif)\n",
        "\n",
        "As we plot the data points, we can recognize height and weight are positively related. But how can we quantify such a relationship?\n",
        "\n",
        "![ab](https://miro.medium.com/max/1050/1*dQnsbaMC-WtRc6LOvIToZg.gif)\n",
        "\n",
        "First, how does a property vary? We probably learn the variance from high school. Let’s introduce its cousin. Sample variance is defined as :\n",
        "\n",
        "![ac](https://miro.medium.com/max/1050/1*2QGDPgiLjHTtERCIe2l1IQ.gif)\n",
        "\n",
        "Note, it is divided by n-1 instead of n in the variance. With a limited size of the samples, the sample mean is biased and correlated with the samples. The average square distance from this mean will be smaller than that from the general population. The sample covariance S², divided by n-1, compensates for the smaller value and can be proven to be an unbiased estimate for variance σ². \n",
        "\n",
        "## **Covariance matrices**\n",
        "\n",
        "Variance measures how a variable varies between itself while covariance is between two variables (a and b).\n",
        "\n",
        "![ad](https://miro.medium.com/max/1050/1*ekRCblqz5Q0ItHb6bsMe8g.gif)\n",
        "\n",
        "We can hold all these possible combinations of covariance in a matrix called the covariance matrix Σ.\n",
        "\n",
        "![ae](https://miro.medium.com/max/1050/1*6121tc3mQ3d-uDy9DQEBXA.jpeg)\n",
        "\n",
        "We can rewrite this in a simple matrix form.\n",
        "\n",
        "![af](https://miro.medium.com/max/1050/1*9lVXIfqEHQ_C-U4ZEzexrw.jpeg)\n",
        "\n",
        "The diagonal elements hold the variances of individual variables (like height) and the non-diagonal elements hold the covariance between two variables. Let’s compute the sample covariance now.\n",
        "\n",
        "![ag](https://miro.medium.com/max/1050/1*i6JeWI_q60qQBt2S4zssBA.gif)\n",
        "\n",
        "![ah](https://miro.medium.com/max/1050/1*ivMbXad2byovZTyg7HgtKQ.gif)\n",
        "\n",
        "The positive sample covariance indicates weight and height are positively correlated. It will be negative if they are negatively correlated and zero if they are independent.\n",
        "\n",
        "![ai](https://miro.medium.com/max/1050/1*BTh8aHR7z7F_QLv44wDmEQ.gif)\n",
        "\n",
        "### **Covariance matrix & SVD**\n",
        "\n",
        "We can use SVD to decompose the sample covariance matrix. Since σ₂ is relatively small compared with σ₁, we can even ignore the σ₂ term. When we train an ML model, we can perform a linear regression on the weight and height to form a new property rather than treating them as two separated and correlated properties (where entangled data usually make model training harder).\n",
        "\n",
        "![aj](https://miro.medium.com/max/1050/1*SCI5suKmi_Zc9ue639UwYw.gif)\n",
        "\n",
        "u₁ has one significant importance. It is the principal component of S.\n",
        "\n",
        "![ak](https://miro.medium.com/max/1050/1*rHqx1h2H6qbNULrMKAMe5Q.gif)\n",
        "\n",
        "There are a few properties about a sample covariance matrix under the context of SVD:\n",
        "\n",
        "* The total variance of the data equals the trace of the sample covariance matrix S which equals the sum of squares of S’s singular values. Equipped with this, we can calculate the ratio of variance lost if we drop smaller σᵢ terms. This reflects the amount of information lost if we eliminate them\n",
        "\n",
        "![al](https://miro.medium.com/max/1050/1*IwKErbFqWgpjsjqT7NSLMg.gif)\n",
        "\n",
        "* The first eigenvector u₁ of S points to the most important direction of the data. In our example, it quantifies the typical ratio between weight and height.\n",
        "\n",
        "![am](https://miro.medium.com/max/659/1*J8jP6pTszPxGCPpE7Fbs1A.jpeg)\n",
        "\n",
        "* The error, calculated as the sum of the perpendicular squared distance from the sample points to u₁, is the minimum when SVD is used.\n",
        "\n",
        "### **Property**\n",
        "\n",
        "Covariance matrices are not only symmetric but they are also positive semidefinite. Because variance is positive or zero, uᵀVu below is always greater or equal zero. By the energy test, V is positive semidefinite.\n",
        "\n",
        "![an](https://miro.medium.com/max/1050/1*ngrrIUIre0An1l6LXZZ1wQ.gif)\n",
        "\n",
        "Therefore,\n",
        "\n",
        "![ao](https://miro.medium.com/max/1050/1*huxsM0xk-4IjATzVH-AcdA.jpeg)\n",
        "\n",
        "Often, after some linear transformation A, we want to know the covariance of the transformed data. This can be calculated with the transformation matrix A and the covariance of the original data.\n",
        "\n",
        "![ap](https://miro.medium.com/max/1050/1*zfcHEnrUVkKqSg2y2MZpAA.gif)\n",
        "\n",
        "### **Correlation matrix**\n",
        "\n",
        "A correlation matrix is a scaled version of the covariance matrix. A correlation matrix standardizes (scale) the variables to have a standard deviation of 1.\n",
        "\n",
        "![aq](https://miro.medium.com/max/1050/1*20um0wwFPjycjwnZr51Fjg.gif)\n",
        "\n",
        "Correlation matrix will be used if variables are in scales of very different magnitudes. Bad scaling may hurt ML algorithms like gradient descent.\n",
        "\n",
        "## **Visualization**\n",
        "\n",
        "So far, we have a lot of equations. Let’s visualize what SVD does and develop the insight gradually. SVD factorizes a matrix A into USVᵀ. Applying A to a vector x (Ax) can be visualized as performing a rotation (Vᵀ), a scaling (S) and another rotation (U) on x.\n",
        "\n",
        "![ar](https://miro.medium.com/max/1050/1*LwmAwpNTGQ_a7n--n3LQpA.jpeg)\n",
        "\n",
        "As shown above, the eigenvector vᵢ of V is transformed into:\n",
        "\n",
        "![as](https://miro.medium.com/max/1050/1*d8c3E0OVl4zKbChXo2GSMA.gif)\n",
        "\n",
        "Or in the full matrix form\n",
        "\n",
        "![at](https://miro.medium.com/max/1050/1*q6aiiqcIx9ES2xaxcnxlyw.gif)\n",
        "\n",
        "## **Insight of SVD**\n",
        "\n",
        "As described before, the SVD can be formulated as\n",
        "\n",
        "![au](https://miro.medium.com/max/1050/1*0sHvvODvpT9536FwBZfO2A.gif)\n",
        "\n",
        "Since uᵢ and vᵢ have unit length, the most dominant factor in determining the significance of each term is the singular value σᵢ. We purposely sort σᵢ in the descending order. If the eigenvalues become too small, we can ignore the remaining terms (+ σᵢuᵢvᵢᵀ + …).\n",
        "\n",
        "![av](https://miro.medium.com/max/1050/1*7Q9pXwIeseO0xLwp7CyESA.gif)\n",
        "\n",
        "This formularization has some interesting implications. For example, we have a matrix contains the return of stock yields traded by different investors.\n",
        "\n",
        "![aw](https://miro.medium.com/max/1050/1*FJXUrl22HERjCUe2dR42mA.gif)\n",
        "\n",
        "As a fund manager, what information can we get out of it? Finding patterns and structures will be the first step. Maybe, we can identify the combination of stocks and investors that have the largest yields. SVD decompose an n × n matrix into r components with the singular value σᵢ demonstrating its significant. Consider this as a way to extract entangled and related properties into fewer principal directions with no correlations.\n",
        "\n",
        "![ax](https://miro.medium.com/max/1050/1*8CRT2ATi3eivfWLTM9Nfbg.jpeg)\n",
        "\n",
        "If data is highly correlated, we should expect many σᵢ values to be small and can be ignored.\n",
        "\n",
        "![ay](https://miro.medium.com/max/1050/1*tPQPCx5mcp4CA6aHZWx9Ig.jpeg)\n",
        "\n",
        "In our previous example, weight and height are highly related. If we have a matrix containing the weight and height of 1000 people, the first component in the SVD decomposition will dominate. The u₁ vector indeed demonstrates the ratio between weight and height among these 1000 people as we discussed before.\n",
        "\n",
        "![az](https://miro.medium.com/max/1050/1*jQNJUMyjard3toQo3sSGrg.jpeg)\n",
        "\n",
        "\n",
        "## **Applications of SVD**\n",
        "\n",
        "### **SVD for Image Compression**\n",
        "\n",
        "How many times have we faced this issue? We love clicking images with our smartphone cameras and saving random photos off the web. And then one day – no space! Image compression helps deal with that headache.\n",
        "\n",
        "It minimizes the size of an image in bytes to an acceptable level of quality. This means that you are able to store more images in the same disk space as compared to before.\n",
        "\n",
        "Image compression takes advantage of the fact that only a few of the singular values obtained after SVD are large. You can trim the three matrices based on the first few singular values and obtain a compressed approximation of the original image. Some of the compressed images are nearly indistinguishable from the original by the human eye.\n",
        "\n",
        "##### **CODE**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "narIRpTj_sQ8",
        "outputId": "e3d0c5bb-5394-4fce-9317-c9ffffe22e4d"
      },
      "source": [
        "from google.colab import files\n",
        "import cv2\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4be81c52-d186-4a03-9d9d-f2f1b41c7db7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4be81c52-d186-4a03-9d9d-f2f1b41c7db7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1_9wAFFiwG9kV3RWnXIo-Rng.jpeg to 1_9wAFFiwG9kV3RWnXIo-Rng (1).jpeg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxLH_hO98hHb",
        "outputId": "ed126744-9f19-4488-d92d-f55eeb05626d"
      },
      "source": [
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# FUNCTION DEFINTIONS:\n",
        "\n",
        "# open the image and return 3 matrices, each corresponding to one channel (R, G and B channels)\n",
        "\n",
        "imOrig = cv2.imread(\"1_9wAFFiwG9kV3RWnXIo-Rng.jpeg\")\n",
        "im = numpy.array(imOrig)\n",
        "\n",
        "aRed = im[:, :, 0]\n",
        "aGreen = im[:, :, 1]\n",
        "aBlue = im[:, :, 2]\n",
        "\n",
        "originalImage = imOrig\n",
        "\n",
        "\n",
        "# compress the matrix of a single channel\n",
        "def compressSingleChannel(channelDataMatrix, singularValuesLimit):\n",
        "    uChannel, sChannel, vhChannel = numpy.linalg.svd(channelDataMatrix)\n",
        "    aChannelCompressed = numpy.zeros((channelDataMatrix.shape[0], channelDataMatrix.shape[1]))\n",
        "    k = singularValuesLimit\n",
        "\n",
        "    leftSide = numpy.matmul(uChannel[:, 0:k], numpy.diag(sChannel)[0:k, 0:k])\n",
        "    aChannelCompressedInner = numpy.matmul(leftSide, vhChannel[0:k, :])\n",
        "    aChannelCompressed = aChannelCompressedInner.astype('uint8')\n",
        "    return aChannelCompressed\n",
        "\n",
        "\n",
        "# MAIN PROGRAM:\n",
        "print('*** Image Compression using SVD - a demo')\n",
        "\n",
        "\n",
        "# image width and height:\n",
        "imageWidth = 512\n",
        "imageHeight = 512\n",
        "\n",
        "# number of singular values to use for reconstructing the compressed image\n",
        "singularValuesLimit = 160\n",
        "\n",
        "aRedCompressed = compressSingleChannel(aRed, singularValuesLimit)\n",
        "aGreenCompressed = compressSingleChannel(aGreen, singularValuesLimit)\n",
        "aBlueCompressed = compressSingleChannel(aBlue, singularValuesLimit)\n",
        "\n",
        "imr = Image.fromarray(aRedCompressed, mode=None)\n",
        "img = Image.fromarray(aGreenCompressed, mode=None)\n",
        "imb = Image.fromarray(aBlueCompressed, mode=None)\n",
        "\n",
        "newImage = Image.merge(\"RGB\", (imr, img, imb))\n",
        "\n",
        "img = cv2.cvtColor(imOrig,cv2.COLOR_BGR2RGB)\n",
        "newImage.show()\n",
        "\n",
        "# CALCULATE AND DISPLAY THE COMPRESSION RATIO\n",
        "mr = imageHeight\n",
        "mc = imageWidth\n",
        "\n",
        "originalSize = mr * mc * 3\n",
        "compressedSize = singularValuesLimit * (1 + mr + mc) * 3\n",
        "\n",
        "print('original size:')\n",
        "print(originalSize)\n",
        "\n",
        "print('compressed size:')\n",
        "print(compressedSize)\n",
        "\n",
        "print('Ratio compressed size / original size:')\n",
        "ratio = compressedSize * 1.0 / originalSize\n",
        "print(ratio)\n",
        "\n",
        "print('Compressed image size is ' + str(round(ratio * 100, 2)) + '% of the original image ')\n",
        "print('DONE - Compressed the image! Over and out!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Image Compression using SVD - a demo\n",
            "original size:\n",
            "786432\n",
            "compressed size:\n",
            "492000\n",
            "Ratio compressed size / original size:\n",
            "0.6256103515625\n",
            "Compressed image size is 62.56% of the original image \n",
            "DONE - Compressed the image! Over and out!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrPQKcu6CJlW"
      },
      "source": [
        "> Now as you can see we have finally compressed our image using SVD. So there are many more applications such as these where SVD is used."
      ]
    }
  ]
}