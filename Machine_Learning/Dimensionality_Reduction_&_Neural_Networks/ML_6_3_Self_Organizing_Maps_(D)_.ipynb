{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_6_3_Self Organizing Maps (D) .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us2Zv0DNgE7V"
      },
      "source": [
        "# **Self-Organizing Map**\n",
        "\n",
        "<h2>What is Self-Organizing Map ?</h2>\n",
        "\n",
        "A self-organizing map is also known as SOM and it was proposed by Kohonen. It is an unsupervised neural network that is trained using unsupervised learning techniques to produce a low dimensional, discretized representation from the input space of the training samples, known as a map and is, therefore, a method to reduce data dimensions. Self-Organizing Maps are very different from other artificial neural networks as they apply competitive learning techniques unlike others using error-correction learning methods such as backpropagation with gradient descent, and use a neighbourhood function to preserve all the topological properties within the input space. \n",
        "\n",
        "Self-Organizing Maps were initially only being used for data visualization, but these days, it has been applied to different problems, including as a solution to the Traveling Salesman Problem as well. Map units or neurons usually form a two-dimensional space and hence a mapping from high dimensional space onto a plane is created. The map retains the calculated relative distance between the points. Points closer to each other within the input space are mapped to the nearby map units in Self-Organizing Maps. Self-Organizing Maps can thus serve as a cluster analyzing tool for high dimensional data. Self-Organizing Maps also have the capability to generalize. During generalization, the network can recognize or characterize inputs that it has never seen as data before. New input is taken up with the map unit and is therefore mapped. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx6Iefmwg1KB"
      },
      "source": [
        "<h2>Uses of Self-Organizing Maps</h2>\n",
        "\n",
        "Self-Organizing Maps provide an advantage in maintaining the structural information from the training data and are not inherently linear. Using Principal Component Analysis on high dimensional data may just cause loss of data when the dimension gets reduced into two. If the data comprises a lot of dimensions and if every dimension preset is useful, in such cases Self-Organizing Maps can be very useful over PCA for dimensionality reduction. Seismic facies analysis generates groups based on the identification of different individual features. This method finds feature organizations in the dataset and forms organized relational clusters. \n",
        "\n",
        "However, these clusters sometimes may or may not have any physical analogs. Therefore a calibration method to relate these clusters to reality is required and Self-Organizing Maps do the job. This calibration method defines the mapping between the groups and the measured physical properties. Text clustering is another important preprocessing step that can be performed through Self-Organizing Maps. It is a method that helps to verify how the present text can be converted into a  mathematical expression for further analysis and processing. Exploratory data analysis and visualization are also the most important applications of Self-Organizing Maps. \n",
        "\n",
        "<p align = \"center\">\n",
        " <img src=\"https://analyticsindiamag.com/wp-content/uploads/2021/09/kohonen1.gif\" />\n",
        " </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH33gtvNhQJy"
      },
      "source": [
        "<h2>Architecture and Working of Self-Organizing Maps </h2>\n",
        "\n",
        "\n",
        "Self-Organizing Maps do not use backpropagation with SGD to update weights, this unsupervised ANN uses competitive learning to update its weights i.e Competition, Cooperation and Adaptation. Each neuron of the output layer is present with a vector with dimension n. The distance between each neuron present at the output layer and the input data is computed. The neuron with the lowest distance is termed as the most suitable fit. Updating the vector of the suitable neuron in the final process is known as adaptation, along with its neighbour in cooperation. After selecting the suitable neuron and its neighbours, we process the neuron to update. The more the distance between the neuron and the input, the more the data grows. \n",
        "\n",
        "To simply explain, learning occurs in the following ways:\n",
        "\n",
        "- Every node is examined to calculate which suitable weights are similar to the input vector. The suitable node is commonly known as the Best Matching Unit.\n",
        "\n",
        "- The neighbourhood value of the Best Matching Unit is then calculated. The number of neighbours tends to decrease over time.\n",
        "\n",
        "- The suitable weight is further rewarded with transitioning into more like the sample vector. The neighbours transition like the sample vector chosen. The closer a node is to the Best Matching Unit, the more its weights get altered and the farther away the neighbour is from the node, the less it learns.\n",
        "\n",
        "- Repeat the second step for N iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axvoeYzniHQe"
      },
      "source": [
        "<h2> Implementation </h2>\n",
        "\n",
        "Self Organizing Maps can easily be implemented in Python using the MiniSom library and Numpy. Below is an example of a Self Organizing Map created on iris data. We will see how to use MiniSom to cluster the seed dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdJ6J1CeiL-c",
        "outputId": "68212951-3b1d-4f1e-92cb-e90048f2aa1f"
      },
      "source": [
        "!pip install minisom"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting minisom\n",
            "  Downloading MiniSom-2.2.9.tar.gz (8.1 kB)\n",
            "Building wheels for collected packages: minisom\n",
            "  Building wheel for minisom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for minisom: filename=MiniSom-2.2.9-py3-none-any.whl size=8594 sha256=144ff61f61fe7ec9ea995f9684f6427ada979dd74d92c6de0a97d799b96f0eb1\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/a1/10/f50b6f4865652eac239a2700de411c3078c27e1318320e494c\n",
            "Successfully built minisom\n",
            "Installing collected packages: minisom\n",
            "Successfully installed minisom-2.2.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_a770n3iZul"
      },
      "source": [
        "from minisom import MiniSom\n",
        " \n",
        "# Initializing neurons and Training\n",
        " \n",
        "n_neurons = 9\n",
        "m_neurons = 9\n",
        "som = MiniSom(n_neurons, m_neurons, data.shape[1], sigma=1.5, learning_rate=.5, \n",
        "              neighborhood_function='gaussian', random_seed=0)\n",
        " \n",
        "som.pca_weights_init(data)\n",
        "som.train(data, 1000, verbose=True)  # random training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxpl10ETieHO"
      },
      "source": [
        "To visualize the result of our training, we can plot the distance map or the U-Matrix, using a pseudocolor where the neurons present in the maps are displayed as an array of cells and the color represents the weighted distance from the neighbouring neurons. On top of the pseudo color we can further add markers that represent the samples mapped into the specific cells:\n",
        "\n",
        "<p align = \"center\">\n",
        "<img src=\"https://analyticsindiamag.com/wp-content/uploads/2021/09/image-19.png\" />\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a7Vm76Zivtj"
      },
      "source": [
        "To understand how the samples are distributed across the map, a scatter chart can be used where each dot represents the coordinates of the winning neuron. A random offset can be added to avoid overlaps between points within the same cell.\n",
        "\n",
        "<p align = \"center\">\n",
        "<img src=\"https://analyticsindiamag.com/wp-content/uploads/2021/09/image-20.png\" />\n",
        "</p>\n",
        "\n",
        "\n",
        "To understand which neurons of the map are activated more often, we can create another pseudocolor plot that reflects the neuro activation frequencies:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<p align = \"center\">\n",
        "<img src=\"https://analyticsindiamag.com/wp-content/uploads/2021/09/image-21.png\" />\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve7idgEnjM_U"
      },
      "source": [
        "plt.figure(figsize=(7, 7))\n",
        "frequencies = som.activation_response(data)\n",
        "plt.pcolor(frequencies.T, cmap='Blues') \n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "#https://analyticsindiamag.com/beginners-guide-to-self-organizing-maps/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEf77zSyjRm0"
      },
      "source": [
        "<h2> Pros And Cons Of Self-Organizing Maps </h2>\n",
        "\n",
        "**Pros**\n",
        "\n",
        "- Data can be easily interpreted and understood with the help of techniques like reduction of dimensionality and grid clustering.\n",
        "\n",
        "- Self-Organizing Maps are capable of handling several types of classification problems while providing a useful, and intelligent summary from the data at the same time.\n",
        "\n",
        "**Cons**\n",
        "\n",
        "- It does not create a generative model for the data and therefore the model does not understand how data is being created.\n",
        "\n",
        "- Self-Organizing Maps do not perform well while working with categorical data and even worse for mixed types of data.\n",
        "\n",
        "- The model preparation time is comparatively very slow and hard to train against the slowly evolving data."
      ]
    }
  ]
}