{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6091c86a",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab207822",
   "metadata": {},
   "source": [
    "The perceptron is a classification algorithm. Basically, it works as a linear binary classifier. It was invented in the late 1950s by Frank Rosenblatt.<br>\n",
    "The perceptron basically works as a threshold function — non-negative outputs are put into one class while negative ones are put into the other class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67661b7",
   "metadata": {},
   "source": [
    "<b>A perceptron has the following components:</b>\n",
    "<br><br>\n",
    "Input nodes<br>\n",
    "Output node<br>\n",
    "An activation function<br>\n",
    "Weights and biases<br>\n",
    "Error function<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a51f14",
   "metadata": {},
   "source": [
    "<b>Input Nodes</b><br>\n",
    "These nodes contain the input to the network. In any iteration — whether testing or training — these nodes are passed the input from our data.<br>\n",
    "<b>Weights and Biases</b><br>\n",
    "These parameters are what we update when we talk about “training” a model. They are initialized to some random value or set to 0 and updated as the training progresses. The bias is analogous to a weight independent of any input node. Basically, it makes the model more flexible, since you can “move” the activation function around.<br>\n",
    "<b>Evaluation</b><br>\n",
    "The output calculation is straightforward.<br>\n",
    "<i>Compute the dot product of the input and weight vector</i><br>\n",
    "<i>Add the bias</i><br>\n",
    "<i>Apply the activation function.</i><br>\n",
    "<b>Activation Function</b><br>\n",
    "This function allows us to fit the output in a way that makes more sense. For example, in the case of a simple classifier, an output of say -2.5 or 8 doesn’t make much sense with regards to classification. If we use something called a sigmoidal activation function, we can fit that within a range of 0 to 1, which can be interpreted directly as a probability of a datapoint belonging to a particular class.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19585f3f",
   "metadata": {},
   "source": [
    "<b>Classification</b><br>\n",
    "We know that a datapoint’s evaluation is expressed by the relation wX + b . We define a threshold (θ) which classifies our data. Generally, this threshold is set to 0 for a perceptron.<br>\n",
    "So points for which wX + b is greater than or equal to 0 will belong to one class while the rest (wX + b is negative) are classified as belonging to the other class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2247045",
   "metadata": {},
   "source": [
    "### The 2D XOR problem\n",
    "In the XOR problem, we are trying to train a model to mimic a 2D XOR function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60844ae",
   "metadata": {},
   "source": [
    "The XOR function\n",
    "The function is defined like so:\n",
    "\n",
    "\n",
    "If we plot it, we get the following chart. This is what we’re trying to classify. The ⊕ (“o-plus”) symbol you see in the legend is conventionally used to represent the XOR boolean operator.\n",
    "\n",
    "\n",
    "Our algorithm —regardless of how it works — must correctly output the XOR value for each of the 4 points. We’ll be modelling this as a classification problem, so Class 1 would represent an XOR value of 1, while Class 0 would represent a value of 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c862b9d0",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd7fbef",
   "metadata": {},
   "source": [
    "Apart from the usual visualization ( matplotlib and seaborn) and numerical libraries (numpy), we’ll use cycle from itertools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328753d7",
   "metadata": {},
   "source": [
    "```from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6470845c",
   "metadata": {},
   "source": [
    "We next create our training data.\n",
    "\n",
    "```\n",
    "train_data = np.array(\n",
    "    [\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [1, 0],\n",
    "        [1, 1]])\n",
    "\n",
    "target_xor = np.array(\n",
    "    [\n",
    "        [0],\n",
    "        [1],\n",
    "        [1],\n",
    "        [0]])\n",
    "\n",
    "target_nand = np.array(\n",
    "    [\n",
    "        [1],\n",
    "        [1],\n",
    "        [1],\n",
    "        [0]])\n",
    "\n",
    "target_or = np.array(\n",
    "    [\n",
    "        [0],\n",
    "        [1],\n",
    "        [1],\n",
    "        [1]])\n",
    "\n",
    "target_and = np.array(\n",
    "    [\n",
    "        [0],\n",
    "        [0],\n",
    "        [0],\n",
    "        [1]])\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f7b319",
   "metadata": {},
   "source": [
    "```def train(self):\n",
    "    \"\"\"\n",
    "    Train a single layer perceptron.\n",
    "    \"\"\"\n",
    "    # the number of consecutive correct classifications\n",
    "    correct_counter = 0\n",
    "\n",
    "    for train, target in cycle(zip(self.train_data, self.target)):\n",
    "        # end if all points are correctly classified\n",
    "        if correct_counter == len(self.train_data):\n",
    "            break\n",
    "\n",
    "        output = self.classify(train)\n",
    "        self.node_val = train\n",
    "\n",
    "        if output == target:\n",
    "            correct_counter += 1\n",
    "        else:\n",
    "            # if incorrectly classified, update weights and reset correct_counter\n",
    "            self.update_weights(target, output)\n",
    "            correct_counter = 0\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffeb71c",
   "metadata": {},
   "source": [
    "```ef _gradient(self, node, exp, output):\n",
    "    \"\"\"\n",
    "    Return the gradient for a weight.\n",
    "    This is the value of delta-w.\n",
    "    \"\"\"\n",
    "    return node * (exp - output)\n",
    "\n",
    "def update_weights(self, exp, output):\n",
    "    \"\"\"\n",
    "    Update weights and bias based on their respective gradients\n",
    "    \"\"\"\n",
    "    for i in range(self.input_nodes):\n",
    "        self.w[i] += self.lr * self._gradient(self.node_val[i], exp, output)\n",
    "\n",
    "    # the value of the bias node can be considered as being 1 and the weight between this node\n",
    "    # and the output node being self.b\n",
    "    self.b += self.lr * self._gradient(1, exp, output)\n",
    "\n",
    "def forward(self, datapoint):\n",
    "    \"\"\"\n",
    "    One forward pass through the perceptron.\n",
    "    Implementation of \"wX + b\".\n",
    "    \"\"\"\n",
    "    return self.b + np.dot(self.w, datapoint)\n",
    "\n",
    "def classify(self, datapoint):\n",
    "    \"\"\"\n",
    "    Return the class to which a datapoint belongs based on\n",
    "    the perceptron's output for that point.\n",
    "    \"\"\"\n",
    "    if self.forward(datapoint) >= 0:\n",
    "        return 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99cb329",
   "metadata": {},
   "source": [
    "```def plot(self, h=0.01):\n",
    "    \"\"\"\n",
    "    Generate plot of input data and decision boundary.\n",
    "    \"\"\"\n",
    "    # setting plot properties like size, theme and axis limits\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.figure(figsize=(20, 20))\n",
    "\n",
    "    plt.axis('scaled')\n",
    "    plt.xlim(-0.1, 1.1)\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "\n",
    "    colors = {\n",
    "        0: \"ro\",\n",
    "        1: \"go\"\n",
    "    }\n",
    "\n",
    "    # plotting the four datapoints\n",
    "    for i in range(len(self.train_data)):\n",
    "        plt.plot([self.train_data[i][0]],\n",
    "                 [self.train_data[i][1]],\n",
    "                 colors[self.target[i][0]],\n",
    "                 markersize=20)\n",
    "\n",
    "    x_range = np.arange(-0.1, 1.1, h)\n",
    "    y_range = np.arange(-0.1, 1.1, h)\n",
    "\n",
    "    # creating a mesh to plot decision boundary\n",
    "    xx, yy = np.meshgrid(x_range, y_range, indexing='ij')\n",
    "    Z = np.array([[self.classify([x, y]) for x in x_range] for y in y_range])\n",
    "\n",
    "    # using the contourf function to create the plot\n",
    "    plt.contourf(xx, yy, Z, colors=['red', 'green', 'green', 'blue'], alpha=0.4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeefe448",
   "metadata": {},
   "source": [
    "To bring everything together, we create a simple Perceptron class with the functions we just discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323212db",
   "metadata": {},
   "source": [
    "```class Perceptron:\n",
    "    \"\"\"\n",
    "    Create a perceptron.\n",
    "    train_data: A 4x2 matrix with the input data.\n",
    "    target: A 4x1 matrix with the perceptron's expected outputs\n",
    "    lr: the learning rate. Defaults to 0.01\n",
    "    input_nodes: the number of nodes in the input layer of the perceptron.\n",
    "        Should be equal to the second dimension of train_data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_data, target, lr=0.01, input_nodes=2):\n",
    "        self.train_data = train_data\n",
    "        self.target = target\n",
    "        self.lr = lr\n",
    "        self.input_nodes = input_nodes\n",
    "\n",
    "        # randomly initialize the weights and set the bias to -1.\n",
    "        self.w = np.random.uniform(size=self.input_nodes)\n",
    "        self.b = -1\n",
    "\n",
    "        # node_val hold the values of each node at a given point of time.\n",
    "        self.node_val = np.zeros(self.input_nodes)\n",
    "\n",
    "    def _gradient(self, node, exp, output):\n",
    "        \"\"\"\n",
    "        Return the gradient for a weight.\n",
    "        This is the value of delta-w.\n",
    "        \"\"\"\n",
    "        return node * (exp - output)\n",
    "\n",
    "    def update_weights(self, exp, output):\n",
    "        \"\"\"\n",
    "        Update weights and bias based on their respective gradients\n",
    "        \"\"\"\n",
    "        for i in range(self.input_nodes):\n",
    "            self.w[i] += self.lr * self._gradient(self.node_val[i], exp, output)\n",
    "\n",
    "        # the value of the bias node can be considered as being 1 and the weight between this node\n",
    "        # and the output node being self.b\n",
    "        self.b += self.lr * self._gradient(1, exp, output)\n",
    "\n",
    "    def forward(self, datapoint):\n",
    "        \"\"\"\n",
    "        One forward pass through the perceptron.\n",
    "        Implementation of \"wX + b\".\n",
    "        \"\"\"\n",
    "        return self.b + np.dot(self.w, datapoint)\n",
    "\n",
    "    def classify(self, datapoint):\n",
    "        \"\"\"\n",
    "        Return the class to which a datapoint belongs based on\n",
    "        the perceptron's output for that point.\n",
    "        \"\"\"\n",
    "        if self.forward(datapoint) >= 0:\n",
    "            return 1\n",
    "\n",
    "        return 0\n",
    "    def plot(self, h=0.01):\n",
    "        \"\"\"\n",
    "        Generate plot of input data and decision boundary.\n",
    "        \"\"\"\n",
    "        # setting plot properties like size, theme and axis limits\n",
    "        sns.set_style('darkgrid')\n",
    "        plt.figure(figsize=(20, 20))\n",
    "\n",
    "        plt.axis('scaled')\n",
    "        plt.xlim(-0.1, 1.1)\n",
    "        plt.ylim(-0.1, 1.1)\n",
    "\n",
    "        colors = {\n",
    "            0: \"ro\",\n",
    "            1: \"go\"\n",
    "        }\n",
    "\n",
    "        # plotting the four datapoints\n",
    "        for i in range(len(self.train_data)):\n",
    "            plt.plot([self.train_data[i][0]],\n",
    "                     [self.train_data[i][1]],\n",
    "                     colors[self.target[i][0]],\n",
    "                     markersize=20)\n",
    "\n",
    "        x_range = np.arange(-0.1, 1.1, h)\n",
    "        y_range = np.arange(-0.1, 1.1, h)\n",
    "\n",
    "        # creating a mesh to plot decision boundary\n",
    "        xx, yy = np.meshgrid(x_range, y_range, indexing='ij')\n",
    "        Z = np.array([[self.classify([x, y]) for x in x_range] for y in y_range])\n",
    "\n",
    "        # using the contourf function to create the plot\n",
    "        plt.contourf(xx, yy, Z, colors=['red', 'green', 'green', 'blue'], alpha=0.4)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train a single layer perceptron.\n",
    "        \"\"\"\n",
    "        # the number of consecutive correct classifications\n",
    "        correct_counter = 0\n",
    "\n",
    "        for train, target in cycle(zip(self.train_data, self.target)):\n",
    "            # end if all points are correctly classified\n",
    "            if correct_counter == len(self.train_data):\n",
    "                break\n",
    "\n",
    "            output = self.classify(train)\n",
    "            self.node_val = train\n",
    "\n",
    "            if output == target:\n",
    "                correct_counter += 1\n",
    "            else:\n",
    "                # if incorrectly classified, update weights and reset correct_counter\n",
    "                self.update_weights(target, output)\n",
    "                correct_counter = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a736c578",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd534a9",
   "metadata": {},
   "source": [
    "Let’s create a perceptron object and train it on the XOR data.\n",
    "\n",
    "```p_xor = Perceptron(train_data, target_xor)\n",
    "p_xor.train()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
