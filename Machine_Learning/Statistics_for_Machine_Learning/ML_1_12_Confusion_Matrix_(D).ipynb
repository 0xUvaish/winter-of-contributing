{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5689757a-2d14-48ae-9c20-f730f07779e1",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix.\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.\n",
    "It allows easy identification of confusion between classes e.g. one class is commonly mislabeled as the other. Most performance measures are computed from the confusion matrix.\n",
    "\n",
    "It gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made.\n",
    "\n",
    "![image1](Assets/Confusion_Matrix1_1.png)\n",
    "\n",
    "Here,\n",
    "* Class 1 : Positive\n",
    "* Class 2 : Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d69f45d-d5a2-4b7d-ad1b-9fff003b0c34",
   "metadata": {},
   "source": [
    "### Definition of the Terms:\n",
    "* Positive (P) : Observation is positive (for example: is an apple).\n",
    "* Negative (N) : Observation is not positive (for example: is not an apple).\n",
    "* True Positive (TP) : Observation is positive, and is predicted to be positive.\n",
    "* False Negative (FN) : Observation is positive, but is predicted negative.\n",
    "* True Negative (TN) : Observation is negative, and is predicted to be negative.\n",
    "* False Positive (FP) : Observation is negative, but is predicted positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b39f5c-15dd-4e20-a8d6-3055982ee95e",
   "metadata": {},
   "source": [
    "### Classification Rate/Accuracy:\n",
    "Classification Rate or Accuracy is given by the relation:\n",
    "![image2](Assets/Confusion_Matrix2_2.png)\n",
    "\n",
    "However, there are problems with accuracy. It assumes equal costs for both kinds of errors. A 99% accuracy can be excellent, good, mediocre, poor or terrible depending upon the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0775936f-e28d-4b38-9295-8be186fa8126",
   "metadata": {},
   "source": [
    "### Recall\n",
    "Recall can be defined as the ratio of the total number of correctly classified positive examples divide to the total number of positive examples. High Recall indicates the class is correctly recognized (small number of FN).\n",
    "\n",
    "Recall is given by the relation:\n",
    "\n",
    "![image3](Assets/Confusion_Matrix3_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabbe640-ec30-4bc8-9a6e-8ae29383ce91",
   "metadata": {},
   "source": [
    "### Precision\n",
    "To get the value of precision we divide the total number of correctly classified positive examples by the total number of predicted positive examples. High Precision indicates an example labeled as positive is indeed positive (small number of FP).\n",
    "Precision is given by the relation:\n",
    "\n",
    "![image4](Assets/Confusion_Matrix4_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca280bc6-d47d-4822-950d-2d38c655a506",
   "metadata": {},
   "source": [
    "### High recall, low precision\n",
    "This means that most of the positive examples are correctly recognized (low FN) but there are a lot of false positives.\n",
    "\n",
    "### Low recall, high precision\n",
    "This shows that we miss a lot of positive examples (high FN) but those we predict as positive are indeed positive (low FP)\n",
    "\n",
    "### F-measure \n",
    "Since we have two measures (Precision and Recall) it helps to have a measurement that represents both of them. We calculate an F-measure which uses Harmonic Mean in place of Arithmetic Mean as it punishes the extreme values more.\n",
    "The F-Measure will always be nearer to the smaller value of Precision or Recall.\n",
    "\n",
    "![image5](Assets/Confusion_Matrix5_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f23b69-8c74-439e-afe2-c0f9580620b2",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97401c1c-4758-4da2-9c76-e7a7dcf1362f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35785f07-7715-4776-be9a-7842846b5ab0",
   "metadata": {},
   "source": [
    "Let’s consider an example now, in which we have infinite data elements of class B and a single element of class A and the model is predicting class A against all the instances in the test data.\n",
    "Here,\n",
    "<br>\n",
    "Precision : 0.0\n",
    "<br>\n",
    "Recall : 1.0\n",
    "<br>\n",
    "Now\n",
    "<br>\n",
    "Arithmetic mean: 0.5\n",
    "<br>\n",
    "Harmonic mean: 0.0\n",
    "<br>\n",
    "When taking the arithmetic mean, it would have 50% correct. Despite being the worst possible outcome! While taking the harmonic mean, the F-measure is 0.\n",
    "\n",
    "\n",
    "\n",
    "Example to interpret confusion matrix:\n",
    "\n",
    "![image6](Assets/Confusion_Matrix6.png)\n",
    "\n",
    "For the simplification of the above confusion matrix i have added all the terms like TP,FP,etc and the row and column totals in the following image:\n",
    "\n",
    "![image7](Assets/Confusion_Matrix7.png)\n",
    "\n",
    "Now,\n",
    "<br>\n",
    "Classification Rate/Accuracy:\n",
    "<br>\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)= (100+50) /(100+5+10+50)= 0.90\n",
    "<br>\n",
    "Recall: Recall gives us an idea about when it’s actually yes, how often does it predict yes.\n",
    "<br>\n",
    "Recall=TP / (TP + FN)=100/(100+5)=0.95\n",
    "<br>\n",
    "Precision: Precsion tells us about when it predicts yes, how often is it correct.\n",
    "<br>\n",
    "Precision = TP / (TP + FP)=100/ (100+10)=0.91\n",
    "<br>\n",
    "F-measure:\n",
    "<br>\n",
    "Fmeasure=(2$\\times$Recall\\timesPrecision)/(Recall+Presision)=\n",
    "<br>\n",
    "(2$\\times$0.95$\\times$0.91)/(0.91+0.95)=0.92"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c290b9-6621-4233-8788-0c57e756c3c4",
   "metadata": {},
   "source": [
    "### Creating a confusion matrix in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a559649-05b0-48fe-bb58-b096b38ee041",
   "metadata": {},
   "source": [
    "Below is the implementation of the confusion matrix with the help of sklearn library.\n",
    "<br>\n",
    "Please Note \n",
    "The 'acutual' and 'predicted' variables in the below code is used just for this example. You can replace the data after creating a machine learning model with the original data and results predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80087944-0668-4a38-a8f1-ed351aa7fdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[4 2]\n",
      " [1 3]]\n",
      "Accuracy Score : 0.7\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73         6\n",
      "           1       0.60      0.75      0.67         4\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.70      0.71      0.70        10\n",
      "weighted avg       0.72      0.70      0.70        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "actual = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0] \n",
    "predicted = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0] \n",
    "results = confusion_matrix(actual, predicted) \n",
    "print('Confusion Matrix :')\n",
    "print(results) \n",
    "print('Accuracy Score :',accuracy_score(actual, predicted))\n",
    "print('Report : ')\n",
    "print(classification_report(actual, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858d21c-3dd0-4f40-872e-a5b73642d06b",
   "metadata": {},
   "source": [
    "References :\n",
    "<br>\n",
    "https://machinelearningmastery.com/confusion-matrix-machine-learning/\n",
    "<br>\n",
    "http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
