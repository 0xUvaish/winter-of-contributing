{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_5_9_Accuracy Scores (D).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITWJghXxPgAd"
      },
      "source": [
        "#**ACCURACY SCORES**\n",
        "\n",
        "Evaluating your machine learning algorithm is an essential part of any project. Your model may give you satisfying results when evaluated using a metric say accuracy_score but may give poor results when evaluated against other metrics such as logarithmic_loss or any other such metric. Most of the times we use classification accuracy to measure the performance of our model, however it is not enough to truly judge our model. In this post, we will cover different types of evaluation metrics available.\n",
        "\n",
        "So here are the types:\n",
        "\n",
        "- Accuracy\n",
        "- Precision\n",
        "- Recall\n",
        "- Area under Curve\n",
        "- F1 Score\n",
        "- Mean Absolute Error\n",
        "- Mean Squared Error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URiw9RtDQa1f"
      },
      "source": [
        "# **Accuracy**\n",
        "\n",
        "It is one of the most straightforward metrics used in machine learning. It defines how accurate your model is. For the problem described above, if you build a model that classifies 90 images accurately, your accuracy is 90% or 0.90."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TVkS-oOQ6Lx"
      },
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Function to calculate accuracy\n",
        "  :param y_true: list of true values\n",
        "  :param y_pred: list of predicted values\n",
        "  :return: accuracy score\n",
        "  \"\"\"\n",
        "  # initialize a simple counter for correct predictions\n",
        "  correct_counter = 0\n",
        "  #loop over all elements of y_true\n",
        "  # and y_pred \"together\"\n",
        "  for yt,yp in zip(y_true, y_pred):\n",
        "    if yt == yp:\n",
        "      correct_counter += 1\n",
        "\n",
        "  return correct_counter / len(y_true)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejEFI3F5R9id"
      },
      "source": [
        "#**Precision**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHfXikIPSHGl"
      },
      "source": [
        "Precision is a metric that quantifies the number of correct positive predictions made.\n",
        "Precision, therefore, calculates the accuracy for the minority class.\n",
        "It is calculated as the ratio of correctly predicted positive examples divided by the total number of positive examples that were predicted.\n",
        "\n",
        "**Precision for Binary Classification**\n",
        "\n",
        "In an imbalanced classification problem with two classes, precision is calculated as the number of true positives divided by the total number of true positives and false positives.\n",
        "\n",
        "- Precision = TruePositives / (TruePositives + FalsePositives)\n",
        "\n",
        "The result is a value between 0.0 for no precision and 1.0 for full or perfect precision.\n",
        "\n",
        "Let’s make this calculation concrete with some examples.\n",
        "\n",
        "Consider a dataset with a 1:100 minority to majority ratio, with 100 minority examples and 10,000 majority class examples.\n",
        "\n",
        "A model makes predictions and predicts 120 examples as belonging to the minority class, 90 of which are correct, and 30 of which are incorrect.\n",
        "\n",
        "The precision for this model is calculated as:\n",
        "\n",
        "- Precision = TruePositives / (TruePositives + FalsePositives)\n",
        "- Precision = 90 / (90 + 30)\n",
        "- Precision = 90 / 120\n",
        "- Precision = 0.75\n",
        "\n",
        "The result is a precision of 0.75, which is a reasonable value but not outstanding.\n",
        "\n",
        "**Precision for Multi-Class Classification**\n",
        "\n",
        "Precision is not limited to binary classification problems.\n",
        "\n",
        "In an imbalanced classification problem with more than two classes, precision is calculated as the sum of true positives across all classes divided by the sum of true positives and false positives across all classes.\n",
        "\n",
        "- Precision = Sum c in C TruePositives_c / Sum c in C (TruePositives_c + FalsePositives_c)\n",
        "\n",
        "For example, we may have an imbalanced multiclass classification problem where the majority class is the negative class, but there are two positive minority classes: class 1 and class 2. Precision can quantify the ratio of correct predictions across both positive classes.\n",
        "\n",
        "Consider a dataset with a 1:1:100 minority to majority class ratio, that is a 1:1 ratio for each positive class and a 1:100 ratio for the minority classes to the majority class, and we have 100 examples in each minority class, and 10,000 examples in the majority class.\n",
        "\n",
        "A model makes predictions and predicts 70 examples for the first minority class, where 50 are correct and 20 are incorrect. It predicts 150 for the second class with 99 correct and 51 incorrect. Precision can be calculated for this model as follows:\n",
        "\n",
        "- Precision = (TruePositives_1 + TruePositives_2) / ((TruePositives_1 + TruePositives_2) + (FalsePositives_1 + FalsePositives_2) )\n",
        "- Precision = (50 + 99) / ((50 + 99) + (20 + 51))\n",
        "- Precision = 149 / (149 + 71)\n",
        "- Precision = 149 / 220\n",
        "- Precision = 0.677\n",
        "\n",
        "We can see that the precision metric calculation scales as we increase the number of minority classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVF1NVzXTI2s",
        "outputId": "ced5d12c-dd4b-4b1a-c36a-f2b72787b9e3"
      },
      "source": [
        "# calculates precision for 1:100 dataset with 90 tp and 30 fp\n",
        "from sklearn.metrics import precision_score\n",
        "# define actual\n",
        "act_pos = [1 for _ in range(100)]\n",
        "act_neg = [0 for _ in range(10000)]\n",
        "y_true = act_pos + act_neg\n",
        "# define predictions\n",
        "pred_pos = [0 for _ in range(10)] + [1 for _ in range(90)]\n",
        "pred_neg = [1 for _ in range(30)] + [0 for _ in range(9970)]\n",
        "y_pred = pred_pos + pred_neg\n",
        "# calculate prediction\n",
        "precision = precision_score(y_true, y_pred, average='binary')\n",
        "print('Precision: %.3f' % precision)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuJDyd5ATXiu"
      },
      "source": [
        "#**Recall**\n",
        "\n",
        "Recall is a metric that quantifies the number of correct positive predictions made out of all positive predictions that could have been made.\n",
        "\n",
        "Unlike precision that only comments on the correct positive predictions out of all positive predictions, recall provides an indication of missed positive predictions.\n",
        "\n",
        "In this way, recall provides some notion of the coverage of the positive class.\n",
        "\n",
        "**Recall for Binary Classification**\n",
        "\n",
        "In an imbalanced classification problem with two classes, recall is calculated as the number of true positives divided by the total number of true positives and false negatives.\n",
        "\n",
        "- Recall = TruePositives / (TruePositives + FalseNegatives)\n",
        "\n",
        "The result is a value between 0.0 for no recall and 1.0 for full or perfect recall.\n",
        "\n",
        "Let’s make this calculation concrete with some examples.\n",
        "\n",
        "As in the previous section, consider a dataset with 1:100 minority to majority ratio, with 100 minority examples and 10,000 majority class examples.\n",
        "\n",
        "A model makes predictions and predicts 90 of the positive class predictions correctly and 10 incorrectly. We can calculate the recall for this model as follows:\n",
        "\n",
        "- Recall = TruePositives / (TruePositives + FalseNegatives)\n",
        "- Recall = 90 / (90 + 10)\n",
        "- Recall = 90 / 100\n",
        "- Recall = 0.9\n",
        "\n",
        "This model has a good recall.\n",
        "\n",
        "**Recall for Multi-Class Classification**\n",
        "\n",
        "Recall is not limited to binary classification problems.\n",
        "\n",
        "In an imbalanced classification problem with more than two classes, recall is calculated as the sum of true positives across all classes divided by the sum of true positives and false negatives across all classes.\n",
        "\n",
        "- Recall = Sum c in C TruePositives_c / Sum c in C (TruePositives_c + FalseNegatives_c)\n",
        "\n",
        "As in the previous section, consider a dataset with a 1:1:100 minority to majority class ratio, that is a 1:1 ratio for each positive class and a 1:100 ratio for the minority classes to the majority class, and we have 100 examples in each minority class, and 10,000 examples in the majority class.\n",
        "\n",
        "A model predicts 77 examples correctly and 23 incorrectly for class 1, and 95 correctly and five incorrectly for class 2. We can calculate recall for this model as follows:\n",
        "\n",
        "- Recall = (TruePositives_1 + TruePositives_2) / ((TruePositives_1 + TruePositives_2) + (FalseNegatives_1 + FalseNegatives_2))\n",
        "- Recall = (77 + 95) / ((77 + 95) + (23 + 5))\n",
        "- Recall = 172 / (172 + 28)\n",
        "- Recall = 172 / 200\n",
        "- Recall = 0.86"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIL94JxeT34D",
        "outputId": "5c38a631-45e6-4434-f318-a6b97f09b9e2"
      },
      "source": [
        "# calculates recall for 1:100 dataset with 90 tp and 10 fn\n",
        "from sklearn.metrics import recall_score\n",
        "# define actual\n",
        "act_pos = [1 for _ in range(100)]\n",
        "act_neg = [0 for _ in range(10000)]\n",
        "y_true = act_pos + act_neg\n",
        "# define predictions\n",
        "pred_pos = [0 for _ in range(10)] + [1 for _ in range(90)]\n",
        "pred_neg = [0 for _ in range(10000)]\n",
        "y_pred = pred_pos + pred_neg\n",
        "# calculate recall\n",
        "recall = recall_score(y_true, y_pred, average='binary')\n",
        "print('Recall: %.3f' % recall)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CJbPt0AUAiO"
      },
      "source": [
        "#**Area Under Curve**\n",
        "\n",
        "Area Under Curve(AUC) is one of the most widely used metrics for evaluation. It is used for binary classification problem. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example. Before defining AUC, let us understand two basic terms :\n",
        "\n",
        "- True Positive Rate (Sensitivity) : True Positive Rate is defined as TP/ (FN+TP). True Positive Rate corresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points.\n",
        "\n",
        "<p align = 'center'>\n",
        " <img src=\"https://miro.medium.com/max/525/1*yw4Y3D7nGNVza2EC2WrOfg.gif\" />\n",
        "</p>\n",
        "\n",
        "- True Negative Rate (Specificity) : True Negative Rate is defined as TN / (FP+TN). False Positive Rate corresponds to the proportion of negative data points that are correctly considered as negative, with respect to all negative data points.\n",
        "\n",
        "<p align = 'center'>\n",
        " <img src=\"https://miro.medium.com/max/580/1*T4PXeK_Hd397C-6ItmLReQ.png\" />\n",
        "</p>\n",
        "\n",
        "- False Positive Rate : False Positive Rate is defined as FP / (FP+TN). False Positive Rate corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points.\n",
        "\n",
        "<p align = 'center'>\n",
        " <img src=\"https://miro.medium.com/max/579/1*857kpm2k2y-eor5Zy3-YeQ.png\" />\n",
        "</p>\n",
        "\n",
        "- False Positive Rate and True Positive Rate both have values in the range [0, 1]. FPR and TPR both are computed at varying threshold values such as (0.00, 0.02, 0.04, …., 1.00) and a graph is drawn. AUC is the area under the curve of plot False Positive Rate vs True Positive Rate at different points in [0, 1].\n",
        "\n",
        "<p align = 'center'>\n",
        " <img src=\"https://miro.medium.com/max/800/1*zFW1Kj3e2X_mmluTW3rVeA.png\" />\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHhTdyQyVibE"
      },
      "source": [
        "#**F1 score**\n",
        "\n",
        "F1 Score is the Harmonic Mean between precision and recall. The range for F1 Score is [0, 1]. It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances).\n",
        "High precision but lower recall, gives you an extremely accurate, but it then misses a large number of instances that are difficult to classify. The greater the F1 Score, the better is the performance of our model. Mathematically, it can be expressed as :\n",
        "\n",
        "\n",
        "<p align = 'center'>\n",
        " <img src=\"https://miro.medium.com/max/239/1*_pYttqYh8w-EpLxMi84H8A.gif\" />\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz9Wf-hCV23t"
      },
      "source": [
        "#**Mean Absolute Error**\n",
        "\n",
        "Mean Absolute Error is the average of the difference between the Original Values and the Predicted Values. It gives us the measure of how far the predictions were from the actual output. However, they don’t gives us any idea of the direction of the error i.e. whether we are under predicting the data or over predicting the data. Mathematically, it is represented as :\n",
        "\n",
        "<p align = 'center'>\n",
        " <img src=\"https://miro.medium.com/max/379/1*qak4Dadzs7pO0hnz4q8O8Q.gif\" />\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzW2F8W0WH9h"
      },
      "source": [
        "#**Mean Squared Error**\n",
        "\n",
        "Mean Squared Error(MSE) is quite similar to Mean Absolute Error, the only difference being that MSE takes the average of the square of the difference between the original values and the predicted values. The advantage of MSE being that it is easier to compute the gradient, whereas Mean Absolute Error requires complicated linear programming tools to compute the gradient. As, we take square of the error, the effect of larger errors become more pronounced then smaller error, hence the model can now focus more on the larger errors.\n",
        "\n",
        "<p align = 'center'>\n",
        " <img src=\"https://miro.medium.com/max/390/1*okvAVQNY6s5cMHxrqUzM5A.gif\" />\n",
        "</p>"
      ]
    }
  ]
}