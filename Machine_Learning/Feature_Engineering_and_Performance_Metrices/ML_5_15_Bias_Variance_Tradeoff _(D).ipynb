{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Variance Tradeoff\n",
    "\n",
    "Whenever we discuss model prediction, it’s important to understand prediction errors (bias and variance). There is a tradeoff between a model’s ability to minimize bias and variance. Gaining a proper understanding of these errors would help us not only to build accurate models but also to avoid the mistake of overfitting and underfitting.\n",
    "\n",
    "## What is Bias?\n",
    "The bias is known as the difference between the prediction of the values by the ML model and the correct value. Being high in biasing gives a large error in training as well as testing data. Its recommended that an algorithm should always be low biased to avoid the problem of underfitting.\n",
    "\n",
    "#### High Bias Techniques\n",
    "Linear Regression, Linear Discriminant Analysis and Logistic Regression\n",
    "#### Low Bias Techniques\n",
    "Decision Trees,  K-nearest neighbours and Gradient Boosting\n",
    "\n",
    "### What is Underfitting of Data?\n",
    "By high bias, the data predicted is in a straight line format, thus not fitting accurately in the data in the data set. Such fitting is known as Underfitting of Data.\n",
    "\n",
    "## What is variance?\n",
    "Variance is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data.\n",
    "\n",
    "#### Low Variance Techniques\n",
    "Linear Regression, Linear Discriminant Analysis, Random Forest, Logistic Regression\n",
    "#### High Variance Techniques\n",
    "Decision Trees,  K-nearest neighbours and Support Vector Machine (SVM)\n",
    "\n",
    "### What is Overfitting of Data?\n",
    "When a model is high on variance, it is then said to as Overfitting of Data. Overfitting is fitting the training set accurately via complex curve and high order hypothesis but is not the solution as the error with unseen data is high.\n",
    "\n",
    "## Errors in Machine Learning\n",
    "There are two main types of errors present in any machine learning model. They are Reducible Errors and Irreducible Errors.\n",
    "\n",
    "- Irreducible errors are errors which will always be present in a machine learning model, because of unknown variables, and whose values cannot be reduced.\n",
    "- Reducible errors are those errors whose values can be further reduced to improve a model. They are caused because our model’s output function does not match the desired output function and can be optimized.\n",
    "\n",
    "We can further divide reducible errors into two: Bias and Variance.\n",
    "\n",
    "## What is Bias-Variance Tradeoff\n",
    "For any model, we have to find the perfect balance between Bias and Variance. This just ensures that we capture the essential patterns in our model while ignoring the noise present it in. This is called Bias-Variance Tradeoff. It helps optimize the error in our model and keeps it as low as possible. An optimized model will be sensitive to the patterns in our data, but at the same time will be able to generalize to new data. In this, both the bias and variance should be low so as to prevent overfitting and underfitting.\n",
    "\n",
    "<img alt=\"Alt text\" src=\"https://4.bp.blogspot.com/-f7ivdW2ttb4/WJdLB_HvEVI/AAAAAAAAF3M/-3GDSzzCBaQUUiuPSAxr6l-hPA6mdzQAQCLcB/s1600/Bias%2BVariance%2BTradeoff.png\">\n",
    "\n",
    "In the image above, the red ball is the target. Any hit close to it is considered as low bias data points. If each subsequent hit is close to the previous hit is considered as low variance cases.\n",
    "\n",
    "The error to complexity graph to show trade-off is given as –\n",
    "\n",
    "<img alt=\"Alt text\" src=\"https://media.geeksforgeeks.org/wp-content/uploads/20200107023418/1_oO0KYF7Z84nePqfsJ9E0WQ.png\">\n",
    "\n",
    "Bias- variance trade-off is needed for the following scenarios-\n",
    "- To overcome underfitting and overfitting condition\n",
    "- To have consistencies in predictions.\n",
    "\n",
    "## Bias, Variance, and Irreducible Error\n",
    "Consider a machine learning model that makes predictions for a predictive modeling task, such as regression or classification. The performance of the model on the task can be described in terms of the prediction error on all examples not used to train the model. We will refer to this as the model error.\n",
    "- Error(Model)\n",
    "The model error can be decomposed into three sources of error: the variance of the model, the bias of the model, and the variance of the irreducible error in the data.\n",
    "- Error(Model) = Variance(Model) + Bias(Model) + Variance(Irreducible Error)\n",
    "\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff Using Python - Part 1\n",
    "\n",
    "Let’s see how we can calculate bias and variance of a model. run this line on the command prompt to get the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the dataset used in this example [here](https://github.com/Ash007-kali/Article-Datasets) (Filename – score.csv).\n",
    "\n",
    "Let’s see how we can determine the Bias and Variance of a model using mlxtend library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required modules\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "#Reading the dataset\n",
    "df = pd.read_csv('score.csv')\n",
    " \n",
    "x = np.array(df.Hours).reshape(-1,1)\n",
    "y = np.array(df.Scores).reshape(-1,1)\n",
    " \n",
    "#Splitting the dataset into train and test set\n",
    "x_train,x_test, y_train, y_test = train_test_split(x,y, test_size = 0.4 , random_state = 0)\n",
    " \n",
    "#Making the model\n",
    "regressor = DecisionTreeRegressor(max_depth = 1)\n",
    " \n",
    "#Fitting the data to the model\n",
    "regressor.fit(x_train,y_train)\n",
    " \n",
    "#Calculating Bias and Variance \n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        regressor, x_train, y_train, x_test, y_test, \n",
    "        loss='mse',\n",
    "        random_seed=1)\n",
    " \n",
    "#Plotting the results\n",
    "x= np.linspace(min(x_train) , max(x_train), 100)\n",
    "plt.plot(x, regressor.predict(x))\n",
    "plt.scatter(x_train , y_train , color = 'red')\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model with a High Bias')\n",
    " \n",
    "print('average Bias: ',avg_bias)\n",
    "print('average Variance: ',avg_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average Bias:  10455.986051700678\n",
    "average Variance:  61.150793197489904"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Alt text\" src=\"https://www.askpython.com/wp-content/uploads/2021/02/Model-with-high-bias.jpeg.webp\">\n",
    "\n",
    "The above plot clearly shows that our model didn’t learn well and hence has a high bias because we set the max depth of the tree as 1. Such a model when evaluated on a test set will yield poor results.We can try playing with the code on a different dataset and using a different model and changing the parameters to get a model that has low bias and low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff Using Python - Part 2\n",
    "\n",
    "Graphical understanding of the Bias-Variance Tradeoff Problem in Machine Learning. Here we will observe what happens to a model as we vary the noise generated by the data generating a signal and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    " \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Creating signal and Plotting It \n",
    "def generate_data(x, noise_threshold=0.1):\n",
    "    signal = np.sin(2 * x) + (1.5 * x) + 0.5\n",
    "    noise = np.random.normal(0, 1, size=len(x)) * noise_threshold\n",
    "    return signal + noise\n",
    "\n",
    "x = np.random.uniform(0, 10, 100)\n",
    "y = generate_data(x=x, noise_threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Alt text\" src=\"https://analyticsindiamag.com/wp-content/uploads/2021/09/image-22.png\">\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a High Bias, Low Variance Model\n",
    "    \n",
    "    signal_x = np.linspace(0, 10, 100)\n",
    "    signal_y = generate_data(x=signal_x, noise_threshold=0.0)\n",
    "    plt.plot(signal_x, signal_y, color='cyan')\n",
    " \n",
    "    x = np.random.uniform(0, 10, 100)\n",
    "    y = generate_data(x=x, noise_threshold=0.7)\n",
    "    plt.scatter(x=x, y=y, s= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Alt text\" src=\"https://analyticsindiamag.com/wp-content/uploads/2021/09/image-23.png\">\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a High Variance, Low Bias Model\n",
    " \n",
    " signal_x = np.linspace(0, 10, 100)\n",
    "    signal_y = generate_data(x=signal_x, noise_threshold=0.0)\n",
    "    plt.plot(signal_x, signal_y, color='cyan')\n",
    " \n",
    "    x = np.random.uniform(0, 10, 100)\n",
    "    y = generate_data(x=x, noise_threshold=0.7)\n",
    "    plt.scatter(x=x, y=y, s= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Alt text\" src=\"https://analyticsindiamag.com/wp-content/uploads/2021/09/image-24.png\">\n",
    "\n",
    "As we can observe, with changing the noise generated by the data generating signals, the levels of variance and bias also do change."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
