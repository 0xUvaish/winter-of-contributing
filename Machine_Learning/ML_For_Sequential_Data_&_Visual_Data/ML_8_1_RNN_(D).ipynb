{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_8_1_RNN(D).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ftod_diGFwNi"
      },
      "source": [
        "# **Recurrent Neural Networks(RNN)**\n",
        "\n",
        "A recurrent neural network (RNN) is a type of artificial neural network which uses sequential data or time series data. These deep learning algorithms are commonly used for ordinal or temporal problems, such as language translation, natural language processing (nlp), speech recognition, and image captioning; they are incorporated into popular applications such as Siri, voice search, and Google Translate. Like feedforward and convolutional neural networks (CNNs), recurrent neural networks utilize training data to learn. They are distinguished by their “memory” as they take information from prior inputs to influence the current input and output. While traditional deep neural networks assume that inputs and outputs are independent of each other, the output of recurrent neural networks depend on the prior elements within the sequence. While future events would also be helpful in determining the output of a given sequence, unidirectional recurrent neural networks cannot account for these events in their predictions.\n",
        "\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://cdn.builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/rnn-vs-fnn.png\" />\n",
        "</p>\n",
        "\n",
        "**Comparison of Recurrent Neural Networks (on the left) and Feedforward Neural Networks (on the right)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkeDq4OPGqam"
      },
      "source": [
        "Recurrent neural networks leverage backpropagation through time (BPTT) algorithm to determine the gradients, which is slightly different from traditional backpropagation as it is specific to sequence data. The principles of BPTT are the same as traditional backpropagation, where the model trains itself by calculating errors from its output layer to its input layer. These calculations allow us to adjust and fit the parameters of the model appropriately. BPTT differs from the traditional approach in that BPTT sums errors at each time step whereas feedforward networks do not need to sum errors as they do not share parameters across each layer.\n",
        "\n",
        "Through this process, RNNs tend to run into two problems, known as exploding gradients and vanishing gradients. These issues are defined by the size of the gradient, which is the slope of the loss function along the error curve. When the gradient is too small, it continues to become smaller, updating the weight parameters until they become insignificant—i.e. 0. When that occurs, the algorithm is no longer learning. Exploding gradients occur when the gradient is too large, creating an unstable model. In this case, the model weights will grow too large, and they will eventually be represented as NaN. One solution to these issues is to reduce the number of hidden layers within the neural network, eliminating some of the complexity in the RNN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flHimDmuHhr8"
      },
      "source": [
        "# **Types of Recurrent Neural Networks**\n",
        "\n",
        "Feedforward networks map one input to one output, and while we’ve visualized recurrent neural networks in this way in the above diagrams, they do not actually have this constraint. Instead, their inputs and outputs can vary in length, and different types of RNNs are used for different use cases, such as music generation, sentiment classification, and machine translation.\n",
        "\n",
        "\n",
        "Type of RNNs:\n",
        "\n",
        "- One to One\n",
        "- One to Many\n",
        "- Many to One\n",
        "- Many to Many\n",
        "\n",
        "\n",
        "\n",
        "<p align = \"center\">\n",
        "<img src= \"https://cdn.builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/Feed-Forward-Neural-Networks.png\" />\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSUHYJ4sJHcI"
      },
      "source": [
        "# **What is Backpropogation?**\n",
        "\n",
        "Backpropagation (BP or backprop, for short) is known as a workhorse algorithm in machine learning. Backpropagation is used for calculating the gradient of an error function with respect to a neural network’s weights. The algorithm works its way backwards through the various layers of gradients to find the partial derivative of the errors with respect to the weights. Backprop then uses these weights to decrease error margins when training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VnVaTh5Im9e"
      },
      "source": [
        "# **Common Activation Functions**\n",
        "\n",
        "An activation function determines whether a neuron should be activated. The nonlinear functions typically convert the output of a given neuron to a value between 0 and 1 or -1 and 1. Some of the most commonly used functions are defined as follows:\n",
        "\n",
        "Sigmoid: This is represented with the formula g(x) = 1/(1 + e^-x).\n",
        "\n",
        "<p align = \"center\">\n",
        "<img src=\"https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_02_13H-RecurrentNeuralNetworks-WHITEBG.png\" />\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "Tanh: This is represented with the formula g(x) = (e^-x - e^-x)/(e^-x + e^-x).\n",
        "\n",
        "\n",
        "<p align = \"center\">\n",
        "<img src=\"https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_02_13I-RecurrentNeuralNetworks-WHITEBG.png\" />\n",
        "</p>\n",
        "\n",
        "Relu: This is represented with the formula g(x) = max(0 , x)\n",
        "\n",
        "\n",
        "<p align = \"center\">\n",
        "<img src=\"https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_02_13J-RecurrentNeuralNetworks-WHITEBG.png\" />\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-I9t-SQKKR5"
      },
      "source": [
        "# **Issues of RNN's**\n",
        "\n",
        "There are two major obstacles RNN’s have had to deal with, but to understand them, you first need to know what a gradient is.\n",
        "\n",
        "A gradient is a partial derivative with respect to its inputs. If you don’t know what that means, just think of it like this: a gradient measures how much the output of a function changes if you change the inputs a little bit.\n",
        "\n",
        "You can also think of a gradient as the slope of a function. The higher the gradient, the steeper the slope and the faster a model can learn. But if the slope is zero, the model stops learning. A gradient simply measures the change in all weights with regard to the change in error.\n",
        "\n",
        "\n",
        "- **EXPLODING GRADIENTS**\n",
        "\n",
        "   - Exploding gradients are when the algorithm, without much reason, assigns a stupidly high importance to the weights. Fortunately, this problem can be easily solved by truncating or squashing the gradients.\n",
        "\n",
        "- **VANISHING GRADIENTS**\n",
        "\n",
        "    - Vanishing gradients occur when the values of a gradient are too small and the model stops learning or takes way too long as a result. This was a major problem in the 1990s and much harder to solve than the exploding gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6gfCzOCLDkS"
      },
      "source": [
        "# **RNN Architectures**\n",
        "\n",
        "**Bidirectional recurrent neural networks (BRNN):** <br/>\n",
        "\n",
        "These are a variant network architecture of RNNs. While unidirectional RNNs can only drawn from previous inputs to make predictions about the current state, bidirectional RNNs pull in future data to improve the accuracy of it. \n",
        "\n",
        "**Long short-term memory (LSTM):** </br>\n",
        "\n",
        "This is a popular RNN architecture, which was introduced by Sepp Hochreiter and Juergen Schmidhuber as a solution to vanishing gradient problem. In their paper they work to address the problem of long-term dependencies. That is, if the previous state that is influencing the current prediction is not in the recent past, the RNN model may not be able to accurately predict the current state. As an example, let’s say we wanted to predict the italicized words in following, “Alice is allergic to nuts. She can’t eat peanut butter.” The context of a nut allergy can help us anticipate that the food that cannot be eaten contains nuts. However, if that context was a few sentences prior, then it would make it difficult, or even impossible, for the RNN to connect the information. To remedy this, LSTMs have “cells” in the hidden layers of the neural network, which have three gates–an input gate, an output gate, and a forget gate. These gates control the flow of information which is needed to predict the output in the network.  For example, if gender pronouns, such as “she”, was repeated multiple times in prior sentences, you may exclude that from the cell state.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://cdn.builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/rnn-three-gates.png\" />\n",
        "</p>\n",
        "\n",
        "**Gated recurrent units (GRUs):** </br>\n",
        "\n",
        "This RNN variant is similar the LSTMs as it also works to address the short-term memory problem of RNN models. Instead of using a “cell state” regulate information, it uses hidden states, and instead of three gates, it has two—a reset gate and an update gate. Similar to the gates within LSTMs, the reset and update gates control how much and which information to retain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPeFkcMvO3vZ",
        "outputId": "6f8fbf17-9f6b-4e7d-8e8b-e47a2c575376"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "(X_train, y_train),(X_test, y_test) = mnist.load_data()  # unpacks images to x_train/x_test and labels to y_train/y_test\n",
        "#Normalizing the data\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "#Initializing the classifier Network\n",
        "classifier = Sequential()\n",
        "\n",
        "#Adding the input LSTM network layer\n",
        "classifier.add(LSTM(128, input_shape=(X_train.shape[1:]), return_sequences=True))\n",
        "classifier.add(Dropout(0.2))\n",
        "\n",
        "#Adding a second LSTM network layer\n",
        "classifier.add(LSTM(128))\n",
        "\n",
        "#Adding a dense hidden layer\n",
        "classifier.add(Dense(64, activation='relu'))\n",
        "classifier.add(Dropout(0.2))\n",
        "\n",
        "#Adding the output layer\n",
        "classifier.add(Dense(10, activation='softmax'))\n",
        "\n",
        "classifier.compile( loss='sparse_categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.001, decay=1e-6),\n",
        "              metrics=['accuracy'] )\n",
        "\n",
        "#Fitting the data to the model\n",
        "classifier.fit(X_train,\n",
        "              y_train,\n",
        "              epochs=3,\n",
        "              validation_data=(X_test, y_test))\n",
        "\n",
        "test_loss, test_acc = classifier.evaluate(X_test, y_test)\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 122s 63ms/step - loss: 0.3404 - accuracy: 0.8930 - val_loss: 0.0960 - val_accuracy: 0.9706\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 119s 63ms/step - loss: 0.1032 - accuracy: 0.9700 - val_loss: 0.0737 - val_accuracy: 0.9804\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 118s 63ms/step - loss: 0.0707 - accuracy: 0.9795 - val_loss: 0.0796 - val_accuracy: 0.9765\n",
            "313/313 [==============================] - 6s 20ms/step - loss: 0.0796 - accuracy: 0.9765\n",
            "Test Loss: 0.07964883744716644\n",
            "Test Accuracy: 0.9764999747276306\n"
          ]
        }
      ]
    }
  ]
}
