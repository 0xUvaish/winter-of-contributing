{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3fc3207",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e31eaf",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, ConvNets have the ability to learn these filters/characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f6e63",
   "metadata": {},
   "source": [
    "Convolution layers are based on the convolution mathematical operation. Convolution layers consist of a set of filters that is just like a two-dimensional matrix of numbers. The filter is then convolved with the input image to produce the output. In each of the convolution layers, we take a filter and slide the filter across the image to perform the convolution operation. The main agenda of the convolution operation is matrix multiplication of the filter values and pixels of the image, and the resultant values are summed to get the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948b3e4c",
   "metadata": {},
   "source": [
    "#### Convolutional neural network architecture\n",
    "\n",
    "* The convolution layer is the building block of CNN. It is responsible for carrying the main portion of the CNN’s computational load.\n",
    "\n",
    "* The pooling layer helps in reducing the spatial size of the representation, which decreases the required amount of computation and weights. The most popular process is the max pooling, which reports the maximum output from the neighborhood. Pooling provides some translation invariance, which means that an object would be recognizable regardless of where it appears on the frame.\n",
    "\n",
    "* The fully connected layer (FC): Neurons in this layer have full connectivity with all neurons in the preceding and succeeding layer, as seen in regular feed-forward neural networks. This is why it can be computed as usual by a matrix multiplication followed by a bias effect. The FC layer helps map the representation between the input and the output.\n",
    "\n",
    "* Layers dealing with nonlinearity\n",
    "    \n",
    "    Since convolution is a linear operation, and images are far from linear, nonlinearity layers are often placed directly after the convolution layer to introduce nonlinearity to the activation map.\n",
    "\n",
    "    There are several types of nonlinear operations, the popular ones being:\n",
    "\n",
    "    * Sigmoid: The sigmoid nonlinearity has the mathematical form f(x) = 1 / 1 + exp(-x). It takes a real-valued number and squeezes it into a range between 0 and 1. Sigmoid suffers a vanishing gradient problem, which is a phenomenon when a local gradient becomes very small and backpropagation leads to killing of the gradient.\n",
    "\n",
    "    * Tanh: Tanh squashes a real-valued number to the range [-1, 1]. Like sigmoid, the activation saturates, but unlike the sigmoid neurons, its output is zero-centered.\n",
    "\n",
    "    * ReLU: The Rectified Linear Unit (ReLU) computes the function ƒ(κ)=max (0,κ). In other words, the activation is simply threshold at zero. In comparison to sigmoid and tanh, ReLU is more reliable and accelerates the convergence by six times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088fbbae",
   "metadata": {},
   "source": [
    "Below you can see python implementation of CNN using Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d55598",
   "metadata": {},
   "source": [
    "## Buidling CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a17c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ab4f28",
   "metadata": {},
   "source": [
    "#### Initialising CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db15f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c7867",
   "metadata": {},
   "source": [
    "#### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec6e7f2",
   "metadata": {},
   "source": [
    "CNN helps us look for specific localized image features like the edges in the image that we can use later in the network Initial layers to detect simple patterns, such as horizontal and vertical edges in an image; and deeper layers detect complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b88d7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Convolution2D(filters = 32,kernel_size = (3, 3), \n",
    "                      input_shape = (64, 64, 3), activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed83282",
   "metadata": {},
   "source": [
    "#### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e5fdc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd2f0fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Convolution2D(filters = 32,kernel_size = (3, 3), activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1564da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba537a5",
   "metadata": {},
   "source": [
    "#### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc1c55db",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87e078",
   "metadata": {},
   "source": [
    "#### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63bd0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 128, activation = \"relu\"))\n",
    "classifier.add(Dense(units = 1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe48d7d",
   "metadata": {},
   "source": [
    "#### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ef7db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc15a05e",
   "metadata": {},
   "source": [
    "## Fitting the CNN to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "838dba54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0966bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b04ce48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_dataset = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                    target_size=(64, 64),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cd3ab7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "testing_dataset = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                                target_size=(64, 64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cf28cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "250/250 [==============================] - 31s 126ms/step - loss: 0.6816 - accuracy: 0.5651 - val_loss: 0.6950 - val_accuracy: 0.5530\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.6130 - accuracy: 0.6620 - val_loss: 0.5499 - val_accuracy: 0.7300\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 0.5508 - accuracy: 0.7201 - val_loss: 0.5328 - val_accuracy: 0.7265\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 0.5373 - accuracy: 0.7286 - val_loss: 0.5239 - val_accuracy: 0.7445\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 0.4952 - accuracy: 0.7569 - val_loss: 0.4979 - val_accuracy: 0.7655\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.4817 - accuracy: 0.7669 - val_loss: 0.5393 - val_accuracy: 0.7320\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.4694 - accuracy: 0.7764 - val_loss: 0.4648 - val_accuracy: 0.7910\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 26s 106ms/step - loss: 0.4508 - accuracy: 0.7881 - val_loss: 0.4696 - val_accuracy: 0.7820\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 28s 110ms/step - loss: 0.4308 - accuracy: 0.7981 - val_loss: 0.4713 - val_accuracy: 0.7960\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 28s 110ms/step - loss: 0.4278 - accuracy: 0.8065 - val_loss: 0.4778 - val_accuracy: 0.7815\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 28s 110ms/step - loss: 0.4016 - accuracy: 0.8150 - val_loss: 0.5068 - val_accuracy: 0.7735\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.3882 - accuracy: 0.8216 - val_loss: 0.4830 - val_accuracy: 0.7960\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.3726 - accuracy: 0.8346 - val_loss: 0.4795 - val_accuracy: 0.7935\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.3602 - accuracy: 0.8407 - val_loss: 0.4609 - val_accuracy: 0.7955\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.3503 - accuracy: 0.8425 - val_loss: 0.5702 - val_accuracy: 0.7860\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.3318 - accuracy: 0.8544 - val_loss: 0.4817 - val_accuracy: 0.8160\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 28s 110ms/step - loss: 0.3260 - accuracy: 0.8558 - val_loss: 0.4779 - val_accuracy: 0.8080\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.3100 - accuracy: 0.8660 - val_loss: 0.5227 - val_accuracy: 0.7940\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 28s 110ms/step - loss: 0.2864 - accuracy: 0.8773 - val_loss: 0.5272 - val_accuracy: 0.8015\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.2794 - accuracy: 0.8821 - val_loss: 0.5521 - val_accuracy: 0.7930\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.2679 - accuracy: 0.8879 - val_loss: 0.5138 - val_accuracy: 0.8045\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 28s 110ms/step - loss: 0.2543 - accuracy: 0.8939 - val_loss: 0.5476 - val_accuracy: 0.7995\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.2345 - accuracy: 0.9034 - val_loss: 0.5406 - val_accuracy: 0.8050\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.2422 - accuracy: 0.8978 - val_loss: 0.5431 - val_accuracy: 0.8010\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.2190 - accuracy: 0.9101 - val_loss: 0.6000 - val_accuracy: 0.7865\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 28s 113ms/step - loss: 0.2073 - accuracy: 0.9160 - val_loss: 0.5500 - val_accuracy: 0.8095\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.1906 - accuracy: 0.9270 - val_loss: 0.6535 - val_accuracy: 0.7910\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 28s 110ms/step - loss: 0.1902 - accuracy: 0.9241 - val_loss: 0.6264 - val_accuracy: 0.8070\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 28s 110ms/step - loss: 0.1640 - accuracy: 0.9342 - val_loss: 0.7079 - val_accuracy: 0.7935\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.1735 - accuracy: 0.9334 - val_loss: 0.6182 - val_accuracy: 0.8005\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.1670 - accuracy: 0.9367 - val_loss: 0.6301 - val_accuracy: 0.7990\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.1581 - accuracy: 0.9386 - val_loss: 0.6665 - val_accuracy: 0.8055\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 28s 112ms/step - loss: 0.1538 - accuracy: 0.9381 - val_loss: 0.6827 - val_accuracy: 0.8025\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.1320 - accuracy: 0.9498 - val_loss: 0.7971 - val_accuracy: 0.7885\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 28s 112ms/step - loss: 0.1358 - accuracy: 0.9498 - val_loss: 0.6478 - val_accuracy: 0.8035\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.1245 - accuracy: 0.9517 - val_loss: 0.7151 - val_accuracy: 0.7950\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.1241 - accuracy: 0.9519 - val_loss: 0.8814 - val_accuracy: 0.7750\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.1243 - accuracy: 0.9534 - val_loss: 0.7542 - val_accuracy: 0.8035\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.1247 - accuracy: 0.9536 - val_loss: 0.7565 - val_accuracy: 0.7965\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.1087 - accuracy: 0.9571 - val_loss: 0.7510 - val_accuracy: 0.8010\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.1067 - accuracy: 0.9581 - val_loss: 0.7825 - val_accuracy: 0.7915\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.1117 - accuracy: 0.9556 - val_loss: 0.7831 - val_accuracy: 0.7955\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.0978 - accuracy: 0.9639 - val_loss: 0.8146 - val_accuracy: 0.7990\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.0908 - accuracy: 0.9663 - val_loss: 0.8691 - val_accuracy: 0.7950\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.0942 - accuracy: 0.9632 - val_loss: 0.8896 - val_accuracy: 0.7895\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.0934 - accuracy: 0.9650 - val_loss: 0.8026 - val_accuracy: 0.8015\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 0.0842 - accuracy: 0.9689 - val_loss: 0.9425 - val_accuracy: 0.7845\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.0845 - accuracy: 0.9668 - val_loss: 0.9221 - val_accuracy: 0.7990\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 0.0871 - accuracy: 0.9681 - val_loss: 0.8044 - val_accuracy: 0.8070\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.0721 - accuracy: 0.9730 - val_loss: 0.8102 - val_accuracy: 0.8055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21c9e16b7f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x = training_dataset, validation_data = testing_dataset, epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e3919f",
   "metadata": {},
   "source": [
    "![image](dataset/test_image/germanshepherd.jpg)\n",
    "<p>\n",
    "This is an image (germanshepherd.jpg) downloaded from google which is for testing whether the model is predicting correctly or not\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9a9ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/test_image/germanshepherd.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "training_dataset.class_indices\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccbe5980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
